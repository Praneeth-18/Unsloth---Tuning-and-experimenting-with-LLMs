{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praneeth-18/Unsloth---Tuning-and-experimenting-with-LLMs/blob/main/unsloth_assignments_tuning_and_experimenting_with_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDplPziqn6Uj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **a.) Finetuning**"
      ],
      "metadata": {
        "id": "lSvhaiC8qnBp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPt5jwjCG6PH",
        "outputId": "f05957f1-7ba2-4954-ac5a-38a497884927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "!pip install -q torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TinyLlama**"
      ],
      "metadata": {
        "id": "PcTN20ttHPTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_tinyllama():\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Print available GPU memory\n",
        "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Load a very small dataset\n",
        "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:10]\")\n",
        "\n",
        "    # Print dataset example to debug\n",
        "    print(\"\\nDataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data with proper dictionary access\n",
        "    def format_data(example):\n",
        "        try:\n",
        "            # Print example structure\n",
        "            print(\"\\nProcessing example:\")\n",
        "            print(example)\n",
        "\n",
        "            return f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in format_data: {e}\")\n",
        "            print(f\"Example structure: {example}\")\n",
        "            raise e\n",
        "\n",
        "    # Process and tokenize dataset\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda examples: {\n",
        "            'input_ids': tokenizer(\n",
        "                format_data(examples),\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )['input_ids'],\n",
        "            'attention_mask': tokenizer(\n",
        "                format_data(examples),\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )['attention_mask']\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # LoRA config\n",
        "    config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"tinyllama_results\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        save_steps=5,\n",
        "        logging_steps=5,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=2,\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Save\n",
        "    model.save_pretrained(\"tinyllama_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_model(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=200)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory first\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Run training with error handling\n",
        "try:\n",
        "    print(\"Starting training...\")\n",
        "    model, tokenizer = train_tinyllama()\n",
        "\n",
        "    print(\"\\nTesting model...\")\n",
        "    test_prompt = \"Explain what is machine learning in simple terms\"\n",
        "    result = test_model(model, tokenizer, test_prompt)\n",
        "    print(\"\\nTest Result:\")\n",
        "    print(result)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e66f532d30444a4ab790830890dcb74f",
            "0b6048d5c63a4a0eaf361cd14a5927fa",
            "8ea43fb9b1654c359d55975b9e64f364",
            "70c6dcd3e1a24835b0fb1c4bc3550e1b",
            "17827b6bc8fa4eacbc276173902abdea",
            "3f55d5f043ef43ea8d1a6074a44a0c42",
            "f8026330927945cf98ebb0aeb3992ca2",
            "8f01e884799b48589c0fd19dc82f6d66",
            "63cf1ab8ad9a48c7803aff94ee59cc92",
            "6ff184c2890d43c688f5133321954129",
            "fc24a91bb6864b398a5395d2218be854"
          ]
        },
        "id": "wicnxsJvHL5q",
        "outputId": "8f155b31-4f21-4adb-9798-7c78042c1518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "GPU memory available: 15.84 GB\n",
            "\n",
            "Dataset example:\n",
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e66f532d30444a4ab790830890dcb74f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing example:\n",
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Which is a species of fish? Tope or Rope', 'context': '', 'response': 'Tope', 'category': 'classification'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Which is a species of fish? Tope or Rope', 'context': '', 'response': 'Tope', 'category': 'classification'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why can camels survive for long without water?', 'context': '', 'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why can camels survive for long without water?', 'context': '', 'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': \"Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\", 'context': '', 'response': 'The name of the third daughter is Alice', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': \"Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\", 'context': '', 'response': 'The name of the third daughter is Alice', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'When was Tomoaki Komorida born?', 'context': 'Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.', 'response': 'Tomoaki Komorida was born on July 10,1981.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'When was Tomoaki Komorida born?', 'context': 'Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.', 'response': 'Tomoaki Komorida was born on July 10,1981.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'If I have more pieces at the time of stalemate, have I won?', 'context': 'Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\\n\\nThe outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.', 'response': \"No. \\nStalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position\", 'category': 'information_extraction'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'If I have more pieces at the time of stalemate, have I won?', 'context': 'Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\\n\\nThe outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.', 'response': \"No. \\nStalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position\", 'category': 'information_extraction'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Given a reference text about Lollapalooza, where does it take place, who started it and what is it?', 'context': \"Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\\n\\nLollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.\", 'response': \"Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.\", 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Given a reference text about Lollapalooza, where does it take place, who started it and what is it?', 'context': \"Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\\n\\nLollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.\", 'response': \"Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.\", 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who gave the UN the land in NY to build their HQ', 'context': '', 'response': 'John D Rockerfeller', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who gave the UN the land in NY to build their HQ', 'context': '', 'response': 'John D Rockerfeller', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why mobile is bad for human', 'context': '', 'response': 'We are always engaged one phone which is not good.', 'category': 'brainstorming'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why mobile is bad for human', 'context': '', 'response': 'We are always engaged one phone which is not good.', 'category': 'brainstorming'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who was John Moses Browning?', 'context': \"John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\\n\\nBrowning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\\n\\nBrowning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.\", 'response': \"John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at the age of 13, and was awarded his first patent when he was 24.\\n\\nHe  designed the first reliable automatic pistol, and the first gas-operated firearm, as well inventing or improving single-shot, lever-action, and pump-action rifles and shotguns.\\n\\nToday, he is most well-known for the M1911 pistol, the Browning Automatic Rifle, and the Auto-5 shotgun, all of which are in still in current production in either their original design, or with minor changes.  His M1911 and Hi-Power pistols designs are some of the most reproduced firearms in the world today.\", 'category': 'information_extraction'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who was John Moses Browning?', 'context': \"John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\\n\\nBrowning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\\n\\nBrowning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.\", 'response': \"John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at the age of 13, and was awarded his first patent when he was 24.\\n\\nHe  designed the first reliable automatic pistol, and the first gas-operated firearm, as well inventing or improving single-shot, lever-action, and pump-action rifles and shotguns.\\n\\nToday, he is most well-known for the M1911 pistol, the Browning Automatic Rifle, and the Auto-5 shotgun, all of which are in still in current production in either their original design, or with minor changes.  His M1911 and Hi-Power pistols designs are some of the most reproduced firearms in the world today.\", 'category': 'information_extraction'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-10-118a6cd8fc56>\", line 121, in <cell line: 119>\n",
            "    model, tokenizer = train_tinyllama()\n",
            "  File \"<ipython-input-10-118a6cd8fc56>\", line 102, in train_tinyllama\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phi**"
      ],
      "metadata": {
        "id": "kljsz9IVHSme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_phi():\n",
        "    print(\"Starting Phi training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Phi model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/phi-1_5\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"microsoft/phi-1_5\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"instruction\": \"Write a Python function to add two numbers\",\n",
        "            \"response\": \"def add_numbers(a, b):\\n    return a + b\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Create a function to check if a number is prime\",\n",
        "            \"response\": \"def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n ** 0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Write a function to reverse a string\",\n",
        "            \"response\": \"def reverse_string(s):\\n    return s[::-1]\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data\n",
        "    def format_data(example):\n",
        "        return f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    def tokenize_function(examples):\n",
        "        formatted_text = format_data(examples)\n",
        "        return tokenizer(\n",
        "            formatted_text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./phi_results\",\n",
        "        num_train_epochs=3,           # Train for a few epochs\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,              # Log every step\n",
        "        save_steps=3,                 # Save every 3 steps\n",
        "        max_steps=10                  # Only train for 10 steps\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./phi_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_phi(model, tokenizer, prompt):\n",
        "    print(f\"\\nTesting with prompt: {prompt}\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nStarting Phi training process...\")\n",
        "    model, tokenizer = train_phi()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_prompts = [\n",
        "        \"Write a Python function to calculate factorial\",\n",
        "        \"Create a function to find the maximum number in a list\"\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        result = test_phi(model, tokenizer, prompt)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Result: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./phi_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ff01fd1e1e14eb8b7ad52f81a1ea3fd",
            "f318fd2b55704b75a633f556c871cd89",
            "5092a2f9bf9a4057bca69edf8007a437",
            "038addf1d7d24f889d32602b8b70526c",
            "8f387f54effd4b3498300df69021125c",
            "b413fe8342774b59a15c01e3dfdf93d0",
            "e0d0fa30bb4c4552bb862dcab154a083",
            "3eef26c3a696472382963211beccd6d1",
            "6c86fde7f35242208271ce9327898b25",
            "67efdf77bc8c43328b31743ef19f8a6e",
            "5dbbe92e8e384538a1edbf509ce3a67f",
            "26ff7fac82334328ad8e9139b81d4d21",
            "edb562214546474f81a017dfeb53857a",
            "7582766f76404a6da354563c1f4d4f9f",
            "cf255189aa2e41e9917e34fd0c169786",
            "01745cf39005455d800e5dcfc46be778",
            "e053161504cd46b8be423010fcc27a64",
            "3c7538ab80234c4d8cdafca327cdcffa",
            "3212c12dac5f4e32aa82d853b4c8fd2f",
            "22076b624b964bedb1b0180e690be2e6",
            "e4d5d88eff954ad5b20fbbb0b1f00d68",
            "afb99e89c44e49a084ff7f71026adfc7",
            "9444ec8101604d348a6b3bf3e7b68aa0",
            "dc0921f1619449ecb9bf156586e83699",
            "0d4371ad347848bca25caf9c5772db32",
            "54f1dd0cd6be482682f83be336482bdb",
            "053a5e9675de4eb0989e8eedf9a8900c",
            "e107f384157b4a3284e610c28b206369",
            "5c4daf55be9c41b8b322d54ae1d06b76",
            "fc934732aa0c45789225381f32c7c32b",
            "09a18e032ae14fbba47bf1bb3021845c",
            "6ddc3e3894b547ab8931c227b6e469cd",
            "c81fef8ee05d4f8fb9d15f85b72da69d",
            "840d19dabe4b45e692253b0d3853451b",
            "9137b435bc9849799f67ddcdc6d42591",
            "61e1b52c19fc4b29868f6e498aaef71e",
            "bc715a33cf6645ceb20cdb7bf0057015",
            "db598876dcc54d778448ed124812308e",
            "fcde8f30b1544180b94ea7d2524b727e",
            "5ceb52a8065d499c868c8385a92ef4f0",
            "7c2dcce21e634d2f9380383d28123b6d",
            "fef9e0b2dbbc454a9d9c5a0ff63d94b0",
            "2998e28451dd4f25ad6b13e4eb93e079",
            "364196c7b36c45fc8e7bf5ffa96b33c8",
            "670e8db05f924cff8585fa10fea01d7d",
            "8fdaebd5faa04eff98f18f0dd0ed6c6e",
            "e955c0cc2ec04ea1bac850e5d4008e76",
            "c6117cb42b484489af2e9cfac4f50439",
            "dd7fba37894b453b99cd778ee83830e3",
            "f83a0d67edcc49278bab7c08253afaaa",
            "4de18af75f6045e28d1443126db45296",
            "62c7a2fd5eac4904ab181a86e44cf7ce",
            "17de9c0fa1d14c4a9ea826aa53ab4472",
            "ed1329222fba4fd991a2fd58452bcf97",
            "2b126a1abfc242dc8122aac0ea6489e9",
            "4da67081204d4cf49fc60276990763b4",
            "4735c288570e42d090a7e45be4017d7b",
            "4af0e3be9aa24f33b01008b62ce1c076",
            "628b59b0b8bb4544998ab983748c941d",
            "6b4ac53c97684a6eb0e934cfe8e4b56f",
            "6e9ddb81447e4f4cb43e9c0e9a125065",
            "4dad24d9376b4fc4b13103a1513a5468",
            "c8dd55165e67446a877948da9ed0c940",
            "513344a9be304eccaf09aef42b907ea0",
            "6b4366e195984b36ad70df3c0e35cf29",
            "efb529e28b204d01ab5772b4637c8c5e",
            "8b6319ed3cbb4dc9a123989cc4759231",
            "f68187a41ef44fb183f67baf1130edca",
            "c789865a92104b7caea4c20995ae7295",
            "df8cd63f56eb49f59bc38b188b8052f4",
            "61b005cc4e1e450484a47e2e9be42858",
            "bd7c367370cf49e2ad34299f69f79b0d",
            "ca706de209724e10acecc1291dffe815",
            "f580fb876e894154a10918cab47f0137",
            "999e1e39c1194cb6971284350aaec16d",
            "20c8d40eef564e0c8f23e332a968501a",
            "d32f06da56e548928818a18cd820b58e",
            "53933d2b1fa348f4b8d01dac412dd646",
            "904b022182cc403990b16fc00c37f113",
            "e2f006fcaedf498a9ec249f02ae52f39",
            "ad12c9cfb3d84efda474bc6504849e05",
            "cfef8cb8e79c48faafb95471cc4837bb",
            "ce66de6833d943aa8074687859873c62",
            "74577a4e5a63479398886394bde72861",
            "825c7a6102b348f9baa3215c0b9a503f",
            "6ceaa9646c984db9b409fe5c696bc3c8",
            "b023a59e6f0c45d5b33912cb42e8e7dd",
            "5592d75806464f378cc46b71b0a26335",
            "b71036d90e8f4e87a6f6b33831256827",
            "30c9643f694842d99e92d98f7cba9c80",
            "af888b4d37d740a688f8f04ea0f60f7e",
            "efceec0674964afb8e3fb603fc07ff4e",
            "9fd432025d544634b0d899e30f6ccba7",
            "5f3969cf212341bfbdfcf48b353ca165",
            "183eaeb871dd43538b3c23b9aafd8897",
            "be357be09166444fabb2cee8591ab334",
            "d971eed6c8d14f14bd578d51862b2725",
            "3f99cabdbd364ea78d1a481e147a5c42",
            "bc848dd6b50f4aea8d95c813402aa616",
            "94f6ff117f004d7f9b212439ebd467ac",
            "337882d306b94ac5a0132bc6c7866898",
            "d4e7573f22934d41a9b6d24f8d2fcdf4",
            "9d38eb288ae045f8af0bbf14441a7720",
            "0be322df609e4b0eacb2a92d9f519d37",
            "01b70c5a3be64bc2bcd1222a72f4efd4",
            "5f9f2af7074c4e338ed9f87d2aef67fc",
            "d1dd3e9a48e5484cba2689d378d269ca",
            "2e3325e34e6848c48f297bdab3f96449",
            "71b2d9a03fe64c24b8cf94dfed5a79bd",
            "3245838e4f854511b7e618724adf4bab"
          ]
        },
        "id": "mqZJdi_dHTQ-",
        "outputId": "c7052e97-d138-4abf-ba34-4c969ca32314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing GPU memory...\n",
            "\n",
            "Starting Phi training process...\n",
            "Starting Phi training setup...\n",
            "Loading Phi model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ff01fd1e1e14eb8b7ad52f81a1ea3fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26ff7fac82334328ad8e9139b81d4d21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9444ec8101604d348a6b3bf3e7b68aa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "840d19dabe4b45e692253b0d3853451b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "670e8db05f924cff8585fa10fea01d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4da67081204d4cf49fc60276990763b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b6319ed3cbb4dc9a123989cc4759231"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53933d2b1fa348f4b8d01dac412dd646"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b71036d90e8f4e87a6f6b33831256827"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Dataset example:\n",
            "{'instruction': 'Write a Python function to add two numbers', 'response': 'def add_numbers(a, b):\\n    return a + b'}\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94f6ff117f004d7f9b212439ebd467ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LoRA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-11-8a5ea4b83429>\", line 144, in <cell line: 142>\n",
            "    model, tokenizer = train_phi()\n",
            "  File \"<ipython-input-11-8a5ea4b83429>\", line 117, in train_phi\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gemma**"
      ],
      "metadata": {
        "id": "lM9XdfiwHVA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_gemma():\n",
        "    print(\"Starting Gemma training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Gemma model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"google/gemma-2b\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple QA dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"question\": \"What is machine learning?\",\n",
        "            \"answer\": \"Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance without being explicitly programmed.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How does photosynthesis work?\",\n",
        "            \"answer\": \"Photosynthesis is the process where plants convert sunlight, water, and carbon dioxide into glucose and oxygen, providing energy for the plant to grow.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What causes earthquakes?\",\n",
        "            \"answer\": \"Earthquakes are caused by the movement of tectonic plates beneath Earth's surface. When these plates move or collide, they release energy in the form of seismic waves.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the theory of relativity?\",\n",
        "            \"answer\": \"The theory of relativity, developed by Einstein, describes how space and time are related and how mass and energy are equivalent (E=mc²).\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data\n",
        "    def format_data(example):\n",
        "        return f\"<start_of_turn>user\\nQuestion: {example['question']}<end_of_turn>\\n<start_of_turn>model\\nAnswer: {example['answer']}<end_of_turn>\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gemma_results\",\n",
        "        num_train_epochs=3,           # Train for a few epochs\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,              # Log every step\n",
        "        save_steps=3,                 # Save every 3 steps\n",
        "        max_steps=10                  # Only train for 10 steps\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./gemma_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_gemma(model, tokenizer, question):\n",
        "    print(f\"\\nTesting with question: {question}\")\n",
        "    prompt = f\"<start_of_turn>user\\nQuestion: {question}<end_of_turn>\\n<start_of_turn>model\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nStarting Gemma training process...\")\n",
        "    model, tokenizer = train_gemma()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_questions = [\n",
        "        \"What is the speed of light?\",\n",
        "        \"How does the human brain work?\",\n",
        "        \"What is quantum computing?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        result = test_gemma(model, tokenizer, question)\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Answer: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./gemma_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Optional: Convert for Ollama\n",
        "def convert_for_ollama():\n",
        "    print(\"\\nConverting for Ollama...\")\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    %cd llama.cpp\n",
        "    !pip install -r requirements.txt\n",
        "    !python convert-lora-to-ggml.py ../gemma_adapter/adapter_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a144390a3580457abaaa9da712fcc9f3",
            "cbaa9194b152495a9d1033a436b63dc7",
            "ef0d493c93814ab597fc2fbebb265902",
            "49745d7c62514c458d007c23edc44950",
            "b4ec5c2e78044bacbc3030896f89bab7",
            "03f9717fbdd04d1ca532734f5ed309e7",
            "3af58b215cc9464c9627861e44a7f49f",
            "34d3e24d1edb4b0f84617b2267af1bb5",
            "a5fef0b232b3437c8646c0b875b7b55e",
            "b32e5cc96553449fbf7d380130ef71b7",
            "4d9f81546546455fbc573984ccc77dae",
            "74efaba1df474425b0ec8174b11df8e0",
            "81322108d6394c93bd9d500ad8ba9702",
            "5bf2c6002c9e465bb3d590b71acab85a",
            "242d137750dc47a1bc644c4a24fffa13",
            "d7b5e24a57d4457abfb361e66cf3b942",
            "0c7bc726c36f4fdfaa4f82fb3009f0d4",
            "d0427f5d2178417786b589b61dacb0bd",
            "3d95d184e2a445c0991531e9f556cf18",
            "db8307f8f27542499278980b43d8e914",
            "7a190779731d4d58911b4ab4a1fc47ac",
            "46477f5e49404d739b413fb28c4dc102",
            "10973e5f20fd4a0dade3fafbf07ba429",
            "3c9f988ac9444e0f9c5e5caaad87c82e",
            "6132c739663a4408b7753feb6c481641",
            "bdd5ccf252874f9b853aeb142abd7890",
            "138b871fc6d04f55af982427a873f345",
            "2a5234b0351c4ed595ce099c0c62dfa8",
            "b75be81addfc442abed11d83375c504d",
            "8b26abac542d474487773ea597a47964",
            "f476fb4738a34d4a93e275784fd630fe",
            "f7fb37564a5b4632aba021e8ec661d28",
            "767eff4aa2274f8a81bb2a1cf27507b3",
            "c1d5ab5e584244b7a3bc5dcc93c1aeba",
            "b2b42ac78c60470ebcad0ad032b50fb4",
            "fcfa9b30308d4ac18e4446ad1359b990",
            "7602823000a34cb1abfe82a1d31c220d",
            "5610f5fe7e9d417084be57d38663d12a",
            "b2c3c9db0f624473808baeadd7dbec67",
            "becfeedb90bb4a75ae6d579a5f4b2216",
            "39af044de00b4aef8b70b857404c43fe",
            "61412b46169c4341b61fc57eb394227b",
            "f46e0c3396504bfe9c5f7a6660d596cd",
            "fd27330b608844afbc0c3a64e61d94f2",
            "d1b6807d3f944d749df9a86cbdcb3684",
            "10d8ef11f1d04e87bc753ee3158b021a",
            "6b9146dcae1048108f2348cc51536d18",
            "d5a5750b88a641fb8080f38083b8e984",
            "a955a08ba64149c5ae365a28376d551c",
            "a1061e2de4b64e7cab9a2cbb403f54ac",
            "68fbe0958cda4832bc8d22bc836d14db",
            "d7bfe46da2e34ca5bbf54a14a1ad7b9f",
            "97b3f03554334f0eafe3273d5c99dcdc",
            "7f6a3c38d3204b35a5455598045c7536",
            "b4b459d783ed4e5cbd30dffb60c450fb",
            "97a9c1ee7e444bf28a2007d12bb6b15e",
            "eb53a979bbeb4d37a6170917f20f7e05",
            "f8300feee1474262b23c5c8c98e67f43",
            "e0550b12362b4362bdd29f462116c956",
            "5b57ce40549445d59c7f86de48109b57",
            "76664fe8073844ada2914011a7001515",
            "58794d4d8814414599c46a42640b266b",
            "538e702a980444c3b18684298540d361",
            "79470505a2b34dc2bf6745c1cd0a793e",
            "f05ac8ce44794b1d90f883541a0f7252",
            "530015905c7a4ee79b897aee4f0768ea",
            "5513847dbf464d6a8268b854f1998d10",
            "e3102b9bceb340328489eddefb7c825c",
            "cb4bd983205b45da82579a6317fc85a3",
            "f066f3f7aafc4d7bbcc75646bf57bd22",
            "1bbadcb769f84bf7b7d220defc0cff77",
            "594ac75ba43c4f5b81525ea5f81e8885",
            "744d223a6e3149a89ebbade951ed27e0",
            "706d2c4f419d404783884cdb5ad83042",
            "331d8a7e051441afa9048fa34cc7c987",
            "4d5cecde050d4821af60ca25df23af6a",
            "b9e31901ee7d4d7ba7753bfa54d20d99",
            "297db9233491414a9c9c5d261c0f78ec",
            "8260fce47c77431abe8d1c96f002b1a5",
            "11ee118068324c79848cce57f8653bea",
            "7ef0b5175fec4b898a43fd1bb4877f50",
            "ea82f8515e564930935279f4c7c5e7dd",
            "2735750b1fe64eacb8ac73bd7008761e",
            "8dc2b1ea752a4343bd3a1db8c5567a39",
            "662d1ecc27b44a659449b44b7cd983d3",
            "83e5f89f1875408faff71ecde58ec02e",
            "5e8810466726409b878d4fc1ab2f6c84",
            "30dc1f30688c48caa6f108d8bfca5c2f",
            "9bf4e33fb1034ce582643311327e2332",
            "8a668622ed5e48ab926cb9faec8b648f",
            "ca291814c539426fb04119b0272b35bc",
            "f2bf69efb1b347a592222569acb41c72",
            "c5a8e5d6cf8044b7addead6024724f21",
            "31f4b2e0d5d541f8ab722f62891e0b84",
            "f7530102d05342f09dab1f2056421a05",
            "61345342587d44d785054cf69fc30839",
            "38099771a2b14bce8cc5d58ebb87427a",
            "c0de310913174a74a2565eba45bcecc0",
            "06a6ca7894404083af08d522cd123aa3",
            "f54106ca4b414142ba8f7a2c0c5297a8",
            "8fb377dbcb76449bb2cfb4ea7df40310",
            "541d7fc28bdc45d59d2943653d91fd6e",
            "c519242363104607ac2b9a5359a7aa80",
            "6d8705652a6e4389a282f9dd8ba2a549",
            "1cdb088fa1a54ecbb1f56a6c98bc756f",
            "542797d75ea943c5b86ca38ab0a53b8e",
            "86b618e87b1e4019b35fb3357f00fe2d",
            "417fd09912974fd6ba7ac698828fd191",
            "eb294bd6bd784a2792d78be92dc28abe",
            "97414d594a804f2bb044694a09ab3917",
            "3f72d5170c7e43faa9a3500d0790b936",
            "45a27e39367e4ccfbb2a9db259d40a28",
            "aa225317c5954e51894eb6b04735407e",
            "d79a52400a854ae29115ce9bbbff1d5c",
            "f0e2f0453aef41a68d89259c681134fc",
            "a6ddd0a2c09b4a73a5015eaf7340df18",
            "346bd39ab10745a5b5f3f281095a7a5d",
            "967325ab7c2f4857bc9cc0482abc68fe",
            "f45fae3675bf4f17883f6d56385ac22d",
            "753b5b4e4167459e934a2ea4022afd14",
            "42031f57db7745deb70cd892b6aebde0",
            "7510370cefe54b3290950def405cdf90",
            "31a580a10e5440a3b33d8157473d6393",
            "d16d006ec4064b50942db8ce35e5a706",
            "3c550e030b36483db9b829113f5f0f71",
            "e91935f0b493446e95f0fdb49812bfd9",
            "4434db387c3747daa09a8a939109f94d",
            "f0106740d1224623ba3f3e233f7e0f07",
            "89fc7eaf0a0a4d648d4139ca67b6cdc4",
            "fe9dd8707c594c538c6297be764da416",
            "b162f89337af4606811be0cf1b34446a",
            "edbdb53c38c84443ba70e30d80ab0f1d"
          ]
        },
        "id": "st7x-r2NHVs2",
        "outputId": "1e3453e1-f783-4422-9d39-f860be6b79fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing GPU memory...\n",
            "\n",
            "Starting Gemma training process...\n",
            "Starting Gemma training setup...\n",
            "Loading Gemma model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a144390a3580457abaaa9da712fcc9f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74efaba1df474425b0ec8174b11df8e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10973e5f20fd4a0dade3fafbf07ba429"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1d5ab5e584244b7a3bc5dcc93c1aeba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1b6807d3f944d749df9a86cbdcb3684"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97a9c1ee7e444bf28a2007d12bb6b15e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5513847dbf464d6a8268b854f1998d10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "297db9233491414a9c9c5d261c0f78ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bf4e33fb1034ce582643311327e2332"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f54106ca4b414142ba8f7a2c0c5297a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f72d5170c7e43faa9a3500d0790b936"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Dataset example:\n",
            "{'question': 'What is machine learning?', 'answer': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance without being explicitly programmed.'}\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7510370cefe54b3290950def405cdf90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LoRA...\n",
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-bb977126c7a6>\", line 135, in <cell line: 133>\n",
            "    model, tokenizer = train_gemma()\n",
            "  File \"<ipython-input-12-bb977126c7a6>\", line 107, in train_gemma\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Qwen-1.5**"
      ],
      "metadata": {
        "id": "AGhug9gYHXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_qwen():\n",
        "    print(\"Starting Qwen training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Qwen model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"Qwen/Qwen1.5-0.5B\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"Qwen/Qwen1.5-0.5B\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple dataset for summarization\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"text\": \"The sun is a massive ball of gas primarily composed of hydrogen and helium. It undergoes nuclear fusion in its core, releasing enormous amounts of energy in the form of light and heat. This energy travels through space and reaches Earth, providing the heat and light necessary for life.\",\n",
        "            \"summary\": \"The sun is a giant gas sphere that produces energy through nuclear fusion, providing Earth with essential light and heat.\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Artificial Intelligence (AI) is a field of computer science focused on creating intelligent machines that can perform tasks typically requiring human intelligence. These tasks include visual perception, speech recognition, decision-making, and language translation. AI systems learn from experience using machine learning algorithms.\",\n",
        "            \"summary\": \"AI is computer technology that mimics human intelligence, performing tasks like recognition, decision-making, and translation through learning algorithms.\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Climate change refers to long-term shifts in global weather patterns and average temperatures. It's primarily caused by human activities releasing greenhouse gases into the atmosphere. These gases trap heat, leading to global warming, rising sea levels, and extreme weather events.\",\n",
        "            \"summary\": \"Climate change involves long-term weather and temperature changes, mainly caused by human-produced greenhouse gases leading to global warming.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data\n",
        "    def format_data(example):\n",
        "        return f\"<|im_start|>user\\nText to summarize: {example['text']}<|im_end|>\\n<|im_start|>assistant\\nSummary: {example['summary']}<|im_end|>\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./qwen_results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        save_steps=3,\n",
        "        max_steps=10\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./qwen_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_qwen(model, tokenizer, text):\n",
        "    print(f\"\\nTesting with text: {text[:100]}...\")  # Print first 100 chars\n",
        "    prompt = f\"<|im_start|>user\\nText to summarize: {text}<|im_end|>\\n<|im_start|>assistant\\nSummary:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nStarting Qwen training process...\")\n",
        "    model, tokenizer = train_qwen()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_texts = [\n",
        "        \"Neural networks are computational systems inspired by biological neural networks in human brains. They consist of interconnected nodes that process and transmit information, allowing the system to learn patterns and make predictions.\",\n",
        "        \"The water cycle, also known as the hydrologic cycle, describes the continuous movement of water on Earth. Water evaporates from surfaces, forms clouds, falls as precipitation, and flows through rivers back to the oceans.\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        result = test_qwen(model, tokenizer, text)\n",
        "        print(f\"\\nOriginal Text: {text[:100]}...\")\n",
        "        print(f\"Generated Summary: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./qwen_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Optional: Convert for Ollama\n",
        "def convert_for_ollama():\n",
        "    print(\"\\nConverting for Ollama...\")\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    %cd llama.cpp\n",
        "    !pip install -r requirements.txt\n",
        "    !python convert-lora-to-ggml.py ../qwen_adapter/adapter_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a10a55da1d614198b5df2a010ee9b249",
            "443d058ac1c94ec5a658bd53c199f47d",
            "c7f2858a9360485ba1e683266659734b",
            "13673a161f1b412a922b1efc2299c246",
            "2e4b034e7e6047dabebcf3ef6ada88bc",
            "b35bfe29b2a7479db75c7fecaff6ce31",
            "ca603beecd8c4188bae055f3041f37eb",
            "470226ed23cc45a3881aa0cf141af72b",
            "08fcdf0d0bf042ab8962ed25a9cdf0f1",
            "aa5780cd672a464992fd576901b6a376",
            "038276d86c3d48a897c4d69a5bb6a242",
            "77f9f4980f6c431b95533d83a10a2e6d",
            "b0b2e545baae453191cf9cc05599df1e",
            "8d77f092fbd24775ba858c977fbf7d70",
            "b3cacc863aab4f7f93f7c5e6867b9cde",
            "37dbeacabcfb4d11903f3c4157840f49",
            "759575d469554ed98d0edd89ba192e5c",
            "c8d941d705044842b79d9d8902188b21",
            "6ae05836655c4e8996e693348ecd21d0",
            "3f0f2f7f5b3f45c0a16b97235df08a75",
            "2d7bd3a0bf734286a1f120d4c1f43a2b",
            "c2b8b8440a8842b5a21f155b923acc15",
            "cdf6c1fad3a243be9264e12d0a3dbd74",
            "4174a1309601415d88d1fb47c05683e9",
            "beb22a6c27d941cfbad3e2c704c197df",
            "e62385ef781d4eb090c85273a50d72bb",
            "e8ecc8c0506a4f33af963943c9b9f3a5",
            "e5651ebb1b5c4f769c6ccc7f0e69a8a3",
            "19c37fabbec04044b96d389e95314e6a",
            "706bb692e03c462a9fc0f51bb6109fe8",
            "c5de8fc2243140c8b839816160731a49",
            "bee5455181bf42e8b0e67d5ef8d6bede",
            "79321d5a2ae24c5e85662cb683f466a6",
            "c8814b4528244041b768f0323f1233bc",
            "e31ba4808b1843079d2e2b9645b98aab",
            "6faea96c03834b13ac15a79c459c1db2",
            "2cf1d9e6145d4f91bca7a808aaea0de3",
            "a53a875d407443278d8292394c8285e1",
            "77fbd595e62e4e4cac7b3da116dd389e",
            "a567990e02e34ade971006ef81726a0a",
            "366fb2127b294d39824a7059a10ee7ec",
            "fe8021f0366d49f7b9026af4a5ff9691",
            "8fd532dfa52c429187b98208382cb77d",
            "2ddc33c2aba84342b582fc1309edf1bd",
            "4da27848345845fcb350928bd3de72b7",
            "f7965e2563924b8c906dce70c8061663",
            "fd635c7d4ce94a24960d465a6fc1cfe8",
            "74a706a7502b44fcb238829dc08ba4b1",
            "bfe554ef97fa40d7873e70ce58e47957",
            "831de406b6b8450e996f4f7b4637dfbf",
            "67fa416a411f4e4d9dc371bd33fa046b",
            "bfd81a4d36c54fc0827da72cf62f94cb",
            "cbc4487a41144235a77d35c0cfa99ea8",
            "d28307b96b174836af0eed4fb0b3d64a",
            "87c649f343994f5eae53015e1fb79e47",
            "5b8a4b1889b8458aa1b6424cc23a2f30",
            "9530ed32242b4e1cb454316f3c41911f",
            "e60a92f4436445af91cb6964f910c8e5",
            "8af05c5ed8614462ad770dcbef7d9c18",
            "2f21990dfd5a4cad83e61c675227729d",
            "efa865a3c87f4da19e761055a029d988",
            "3bc1195b6a834235ab717fd9c35aaa74",
            "12786f16a0814fccb9415ec7b5ecd441",
            "3091179bef4f4d9a8d9ed3b25aa64fc9",
            "6349b110c6514954b65b10532bde5cfd",
            "facb1f1072134053a5861973bb0b83ba",
            "338ceac9f13f49cf9b556ffbdcf9c395",
            "8b66cf49f8bd4b208e385310da5a6103",
            "f7abb9b7b71446cea39cfe75ff0e1ec8",
            "103fbeffa314496db4b9ef6d4dfd1591",
            "4c4c407059ce4be88ec301a57eb2488c",
            "791bd8ca7258424994bd63b5793a6cf6",
            "a5f9ee8e01194524895a744eebcec3c3",
            "af3c4796a69d4df59228077abe00854a",
            "70af9ccb3b1a4119a92a3a271db3b3eb",
            "92ce85d57fde467a8c9cc0d5f698e913",
            "242ee10cfae848519df6a7f216593f4b",
            "780eb17b2903466985836009de01dec9",
            "f865d08448844cb981cd4735071fe27b",
            "ca4e5d3faf63495085c98e5ba3822fb9",
            "da0e2a0ad8a340d4a1c6ae6e6217e0b9",
            "6576542d1eb14e51a4a8109f74b98931",
            "96a6c64dd774460688eb7e5508e3172b",
            "2f04f405e24e4cd3b568df7ec7356484",
            "712f301aca0f43379a02f650e72dbccc",
            "d462392e7d064b718f0c93a63975b229",
            "cb4ae771ef0d443c91ee5972c3eae10a",
            "afbd7f59932743eea650eff965f2fbcb"
          ]
        },
        "id": "JpV04C7wHYhH",
        "outputId": "b07b3760-b7ac-4981-b137-a02510fb94cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing GPU memory...\n",
            "\n",
            "Starting Qwen training process...\n",
            "Starting Qwen training setup...\n",
            "Loading Qwen model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a10a55da1d614198b5df2a010ee9b249"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77f9f4980f6c431b95533d83a10a2e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdf6c1fad3a243be9264e12d0a3dbd74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8814b4528244041b768f0323f1233bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4da27848345845fcb350928bd3de72b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b8a4b1889b8458aa1b6424cc23a2f30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "338ceac9f13f49cf9b556ffbdcf9c395"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Dataset example:\n",
            "{'text': 'The sun is a massive ball of gas primarily composed of hydrogen and helium. It undergoes nuclear fusion in its core, releasing enormous amounts of energy in the form of light and heat. This energy travels through space and reaches Earth, providing the heat and light necessary for life.', 'summary': 'The sun is a giant gas sphere that produces energy through nuclear fusion, providing Earth with essential light and heat.'}\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "780eb17b2903466985836009de01dec9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LoRA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-13-7c3b3ffeebf2>\", line 135, in <cell line: 133>\n",
            "    model, tokenizer = train_qwen()\n",
            "  File \"<ipython-input-13-7c3b3ffeebf2>\", line 107, in train_qwen\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mistral-7B-Instruct-v0.2**"
      ],
      "metadata": {
        "id": "TU1gUYNWI7hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_mistral():\n",
        "    print(\"Starting Mistral training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Mistral model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"mistralai/Mistral-7B-v0.1\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple instruction dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"instruction\": \"Explain quantum computing\",\n",
        "            \"response\": \"Quantum computing uses quantum mechanics principles like superposition and entanglement to perform computations. Unlike classical computers that use bits (0 or 1), quantum computers use quantum bits or qubits that can exist in multiple states simultaneously, potentially solving complex problems much faster.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"What is machine learning?\",\n",
        "            \"response\": \"Machine learning is a branch of artificial intelligence where computer systems learn and improve from experience without explicit programming. They identify patterns in data to make predictions and decisions, becoming more accurate over time.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"How does blockchain work?\",\n",
        "            \"response\": \"Blockchain is a decentralized digital ledger that records transactions across a network of computers. Each block contains transaction data and links to the previous block, creating a chain. The system is secure because changes require consensus from the network, making it difficult to alter records.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data with Mistral chat template\n",
        "    def format_data(example):\n",
        "        return f\"<s>[INST] {example['instruction']} [/INST] {example['response']}</s>\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\"\n",
        "        ],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()  # Print trainable parameters\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./mistral_results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        save_steps=3,\n",
        "        max_steps=10,\n",
        "        fp16=True,  # Use mixed precision\n",
        "        optim=\"paged_adamw_8bit\"  # Use 8-bit optimizer\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./mistral_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_mistral(model, tokenizer, instruction):\n",
        "    print(f\"\\nTesting with instruction: {instruction}\")\n",
        "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Memory management function\n",
        "def clear_memory():\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nClearing memory...\")\n",
        "    clear_memory()\n",
        "\n",
        "    print(\"\\nStarting Mistral training process...\")\n",
        "    model, tokenizer = train_mistral()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_instructions = [\n",
        "        \"Explain how solar panels work\",\n",
        "        \"What is the theory of evolution?\",\n",
        "        \"How does the internet work?\"\n",
        "    ]\n",
        "\n",
        "    for instruction in test_instructions:\n",
        "        result = test_mistral(model, tokenizer, instruction)\n",
        "        print(f\"\\nInstruction: {instruction}\")\n",
        "        print(f\"Response: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./mistral_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Convert for Ollama\n",
        "def convert_for_ollama():\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    %cd llama.cpp\n",
        "    !pip install -r requirements.txt\n",
        "    !python convert-lora-to-ggml.py ../mistral_adapter/adapter_config.json\n",
        "\n",
        "    # Create Modelfile with proper string formatting\n",
        "    modelfile_content = '''\n",
        "FROM mistral:latest\n",
        "ADAPTER ./ggml-adapter-model.bin\n",
        "TEMPLATE \"\"\"<s>[INST] {{ .Prompt }} [/INST]\"\"\"\n",
        "PARAMETER stop \"[INST]\"\n",
        "PARAMETER stop \"</s>\"\n",
        "'''\n",
        "\n",
        "    with open(\"Modelfile\", \"w\") as f:\n",
        "        f.write(modelfile_content)\n",
        "\n",
        "    print(\"Created Modelfile for Ollama\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSSkpKsSI8cJ",
        "outputId": "092ef1e3-793e-43b9-d30f-6494cbcf0ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clearing memory...\n",
            "GPU Memory available: 15.84 GB\n",
            "\n",
            "Starting Mistral training process...\n",
            "Starting Mistral training setup...\n",
            "Loading Mistral model...\n",
            "\n",
            "An error occurred: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n",
            "403 Client Error. (Request ID: Root=1-67229ce1-08c0444f27bd206e0e06af16;283224c8-c909-4d2c-9460-5e77c7cca34e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
            "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1855, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1752, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1674, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
            "    raise GatedRepoError(message, response) from e\n",
            "huggingface_hub.utils._errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-67229ce1-08c0444f27bd206e0e06af16;283224c8-c909-4d2c-9460-5e77c7cca34e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
            "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-15-9fd45cada668>\", line 147, in <cell line: 142>\n",
            "    model, tokenizer = train_mistral()\n",
            "  File \"<ipython-input-15-9fd45cada668>\", line 19, in train_mistral\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 524, in from_pretrained\n",
            "    config, kwargs = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 976, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 420, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n",
            "403 Client Error. (Request ID: Root=1-67229ce1-08c0444f27bd206e0e06af16;283224c8-c909-4d2c-9460-5e77c7cca34e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
            "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSd0Ppy4JUzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **b.) Continued pretraining**"
      ],
      "metadata": {
        "id": "OMwfB_aDOE0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHVNcustOPNb",
        "outputId": "9f552c61-e011-409e-f3d3-ee74d0de4a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2024.10.7-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth-zoo (from unsloth)\n",
            "  Downloading unsloth_zoo-2024.10.5-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.0+cu121)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.1)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.1)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: transformers<4.45.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.44.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.0.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.34.2)\n",
            "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.24.7)\n",
            "Collecting hf-transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.45.0->unsloth) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<4.45.0->unsloth) (0.19.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (13.9.3)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth) (0.2.0)\n",
            "Downloading unsloth-2024.10.7-py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2024.10.5-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: triton, shtab, hf-transfer, xformers, tyro, trl, unsloth-zoo, unsloth\n",
            "Successfully installed hf-transfer-0.1.8 shtab-1.7.1 triton-3.1.0 trl-0.11.1 tyro-0.8.14 unsloth-2024.10.7 unsloth-zoo-2024.10.5 xformers-0.0.28.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for Hugging Face token securely\n",
        "hf_token = getpass(\"Please enter your Hugging Face token: \")\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=hf_token)\n",
        "\n",
        "# Now load your model\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B\"  # Replace with the model name\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=512,\n",
        "    dtype=\"float16\",\n",
        "    load_in_4bit=True,\n",
        "    token=hf_token  # Pass token for private model access\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "G1-nmr1jOWvD",
        "outputId": "5d5a26e6-0f6e-4374-ff8f-96fd048b24ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your Hugging Face token: ··········\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mistralai/Mistral-7B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-64aaccdd55b5>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistralai/Mistral-7B\"\u001b[0m  \u001b[0;31m# Replace with the model name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0;34mf\"to obtain the latest transformers build, then restart this session.\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 ) \n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoconfig_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpeft_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mistralai/Mistral-7B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define your model name (e.g., a smaller Hugging Face model for Colab)\n",
        "model_name = \"mistralai/Mistral-7B\"  # Replace with the desired model\n",
        "\n",
        "# Load the model and tokenizer with settings optimized for Colab\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=512,\n",
        "    dtype=\"float16\",  # Helps reduce memory usage\n",
        "    load_in_4bit=True,  # Further reduces memory load\n",
        "    token=os.environ[\"HUGGINGFACE_TOKEN\"]  # Fetches token from environment\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "unUfXYIsQaBH",
        "outputId": "ac206717-0aa5-4f6a-f62e-9c63e3d1e6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-138fa4627c1c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Helps reduce memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Further reduces memory load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HUGGINGFACE_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Fetches token from environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXXuqaNpRsC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **c.) Chat templates**"
      ],
      "metadata": {
        "id": "41xSM1jQp14B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "!pip install transformers datasets accelerate bitsandbytes trl peft\n",
        "!pip install -q pytorch_lightning wandb\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import json\n",
        "\n",
        "# 1. Basic Setup and Configurations\n",
        "def setup_model_and_tokenizer(base_model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "    \"\"\"Setup the model and tokenizer with appropriate configurations\"\"\"\n",
        "\n",
        "    # QLoRA configurations\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=False\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 2. Chat Templates for Different Tasks\n",
        "\n",
        "def classification_template(example: Dict) -> str:\n",
        "    \"\"\"Template for classification tasks\"\"\"\n",
        "    return f\"\"\"<|system|>You are a helpful AI assistant that performs classification tasks.</s>\n",
        "<|user|>{example['text']}</s>\n",
        "<|assistant|>The category for this text is: {example['label']}</s>\"\"\"\n",
        "\n",
        "def conversation_template(example: Dict) -> str:\n",
        "    \"\"\"Template for conversational data\"\"\"\n",
        "    dialog = \"\"\n",
        "    for turn in example['conversation']:\n",
        "        if turn['role'] == 'user':\n",
        "            dialog += f\"<|user|>{turn['content']}</s>\"\n",
        "        else:\n",
        "            dialog += f\"<|assistant|>{turn['content']}</s>\"\n",
        "    return f\"<|system|>You are a helpful AI assistant.</s>{dialog}\"\n",
        "\n",
        "# 3. Dataset Preparation Functions\n",
        "\n",
        "def prepare_classification_dataset(dataset_name: str, text_column: str, label_column: str):\n",
        "    \"\"\"Prepare a dataset for classification fine-tuning\"\"\"\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    def format_example(example):\n",
        "        return {\n",
        "            'text': example[text_column],\n",
        "            'label': example[label_column]\n",
        "        }\n",
        "\n",
        "    return dataset.map(format_example)\n",
        "\n",
        "def prepare_conversation_dataset(conversations: List[Dict]):\n",
        "    \"\"\"Prepare a dataset for conversational fine-tuning\"\"\"\n",
        "    formatted_data = []\n",
        "    for conv in conversations:\n",
        "        formatted_data.append({\n",
        "            'conversation': conv\n",
        "        })\n",
        "    return formatted_data\n",
        "\n",
        "# 4. Extended Context Size Implementation\n",
        "\n",
        "def extend_context_size(model, tokenizer, target_context_size=8192):\n",
        "    \"\"\"Extend the context size of the model\"\"\"\n",
        "    model.config.max_position_embeddings = target_context_size\n",
        "    tokenizer.model_max_length = target_context_size\n",
        "\n",
        "    # Interpolate position embeddings\n",
        "    orig_pos_emb = model.get_input_embeddings().weight\n",
        "    new_pos_emb = torch.nn.Parameter(\n",
        "        torch.zeros(target_context_size, orig_pos_emb.size(1))\n",
        "    )\n",
        "\n",
        "    # Linear interpolation\n",
        "    steps = target_context_size // orig_pos_emb.size(0)\n",
        "    for i in range(target_context_size):\n",
        "        orig_idx = i // steps\n",
        "        next_idx = min(orig_idx + 1, orig_pos_emb.size(0) - 1)\n",
        "        alpha = (i % steps) / steps\n",
        "        new_pos_emb.data[i] = (1 - alpha) * orig_pos_emb.data[orig_idx] + alpha * orig_pos_emb.data[next_idx]\n",
        "\n",
        "    model.get_input_embeddings().weight = new_pos_emb\n",
        "    return model, tokenizer\n",
        "\n",
        "# 5. Multi-Dataset Fine-tuning\n",
        "\n",
        "def prepare_mixed_dataset(datasets: List[Dict]):\n",
        "    \"\"\"Combine multiple datasets for single fine-tuning\"\"\"\n",
        "    combined_data = []\n",
        "\n",
        "    for dataset_info in datasets:\n",
        "        data = dataset_info['data']\n",
        "        template_fn = dataset_info['template']\n",
        "\n",
        "        formatted_data = [template_fn(example) for example in data]\n",
        "        combined_data.extend(formatted_data)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# 6. Training Setup\n",
        "\n",
        "def setup_training(model, dataset, output_dir=\"./results\"):\n",
        "    \"\"\"Setup LoRA and training arguments\"\"\"\n",
        "\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Prepare model for training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.001,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    return model, training_args\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# 1. Classification Example\n",
        "\"\"\"\n",
        "model, tokenizer = setup_model_and_tokenizer()\n",
        "\n",
        "# Load and prepare classification dataset\n",
        "classification_data = prepare_classification_dataset(\n",
        "    \"emotion\",\n",
        "    text_column=\"text\",\n",
        "    label_column=\"label\"\n",
        ")\n",
        "\n",
        "# Format data with classification template\n",
        "formatted_data = [classification_template(example) for example in classification_data]\n",
        "\n",
        "# Setup and train\n",
        "model, training_args = setup_training(model, formatted_data)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_data\n",
        ")\n",
        "trainer.train()\n",
        "\"\"\"\n",
        "\n",
        "# 2. Conversational Example\n",
        "\"\"\"\n",
        "# Example conversation data\n",
        "conversations = [\n",
        "    {\n",
        "        'conversation': [\n",
        "            {'role': 'user', 'content': 'Hello!'},\n",
        "            {'role': 'assistant', 'content': 'Hi there! How can I help you today?'}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "conv_data = prepare_conversation_dataset(conversations)\n",
        "formatted_conv_data = [conversation_template(example) for example in conv_data]\n",
        "\"\"\"\n",
        "\n",
        "# 3. Extended Context Example\n",
        "\"\"\"\n",
        "model, tokenizer = setup_model_and_tokenizer()\n",
        "model, tokenizer = extend_context_size(model, tokenizer, target_context_size=8192)\n",
        "\"\"\"\n",
        "\n",
        "# 4. Multi-Dataset Example\n",
        "\"\"\"\n",
        "datasets = [\n",
        "    {\n",
        "        'data': classification_data,\n",
        "        'template': classification_template\n",
        "    },\n",
        "    {\n",
        "        'data': conv_data,\n",
        "        'template': conversation_template\n",
        "    }\n",
        "]\n",
        "\n",
        "combined_dataset = prepare_mixed_dataset(datasets)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CdaJw5p1p6JO",
        "outputId": "4010a9d2-0855-474f-cacd-6cb7225383f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting trl\n",
            "  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, tokenizers, bitsandbytes, transformers, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.44.1 datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 tokenizers-0.20.1 transformers-4.46.1 trl-0.12.0 xxhash-3.5.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.6/890.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndatasets = [\\n    {\\n        'data': classification_data,\\n        'template': classification_template\\n    },\\n    {\\n        'data': conv_data,\\n        'template': conversation_template\\n    }\\n]\\n\\ncombined_dataset = prepare_mixed_dataset(datasets)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6wXB4DY0qBYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **d.) Reward Modeling**"
      ],
      "metadata": {
        "id": "fffu_ysNqhf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch transformers datasets accelerate bitsandbytes trl peft wandb\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import DPOTrainer\n",
        "import wandb\n",
        "\n",
        "# 1. Basic Setup\n",
        "def setup_model_and_tokenizer(base_model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"):\n",
        "    \"\"\"Initialize model and tokenizer with 4-bit quantization\"\"\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=False\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 2. ORPO Implementation\n",
        "\n",
        "@dataclass\n",
        "class ORPOConfig:\n",
        "    \"\"\"Configuration for ORPO training\"\"\"\n",
        "    beta: float = 0.1  # Temperature parameter\n",
        "    lambda_entropy: float = 0.1  # Entropy regularization coefficient\n",
        "    gamma: float = 0.99  # Discount factor\n",
        "    clip_range: float = 0.2  # Policy clip range\n",
        "\n",
        "class ORPOTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        ref_model,\n",
        "        tokenizer,\n",
        "        reward_model,\n",
        "        config: ORPOConfig,\n",
        "        training_args: TrainingArguments\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.ref_model = ref_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.reward_model = reward_model\n",
        "        self.config = config\n",
        "        self.training_args = training_args\n",
        "\n",
        "    def compute_reward(self, responses):\n",
        "        \"\"\"Compute rewards using the reward model\"\"\"\n",
        "        with torch.no_grad():\n",
        "            rewards = self.reward_model(responses)\n",
        "        return rewards\n",
        "\n",
        "    def compute_kl_divergence(self, policy_logits, ref_logits):\n",
        "        \"\"\"Compute KL divergence between policy and reference model\"\"\"\n",
        "        policy_probs = torch.softmax(policy_logits, dim=-1)\n",
        "        ref_probs = torch.softmax(ref_logits, dim=-1)\n",
        "        kl_div = torch.sum(policy_probs * (torch.log(policy_probs) - torch.log(ref_probs)), dim=-1)\n",
        "        return kl_div\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Single ORPO training step\"\"\"\n",
        "        # Get model outputs\n",
        "        policy_outputs = self.model(batch[\"input_ids\"])\n",
        "        ref_outputs = self.ref_model(batch[\"input_ids\"])\n",
        "\n",
        "        # Compute rewards\n",
        "        rewards = self.compute_reward(policy_outputs.logits)\n",
        "\n",
        "        # Compute KL divergence\n",
        "        kl_div = self.compute_kl_divergence(policy_outputs.logits, ref_outputs.logits)\n",
        "\n",
        "        # Compute ORPO loss\n",
        "        advantages = rewards - self.config.beta * kl_div\n",
        "\n",
        "        # Policy loss with clipping\n",
        "        ratio = torch.exp(policy_outputs.logits - ref_outputs.logits)\n",
        "        clipped_ratio = torch.clamp(ratio, 1 - self.config.clip_range, 1 + self.config.clip_range)\n",
        "\n",
        "        policy_loss = -torch.min(\n",
        "            ratio * advantages,\n",
        "            clipped_ratio * advantages\n",
        "        ).mean()\n",
        "\n",
        "        # Entropy regularization\n",
        "        entropy = -(torch.softmax(policy_outputs.logits, dim=-1) *\n",
        "                   torch.log_softmax(policy_outputs.logits, dim=-1)).sum(dim=-1).mean()\n",
        "\n",
        "        total_loss = policy_loss - self.config.lambda_entropy * entropy\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def train(self, train_dataset):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "        for epoch in range(self.training_args.num_train_epochs):\n",
        "            total_loss = 0\n",
        "            for batch in train_dataset:\n",
        "                loss = self.train_step(batch)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Optimization step\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.training_args.max_grad_norm)\n",
        "                self.training_args.optimizer.step()\n",
        "                self.training_args.optimizer.zero_grad()\n",
        "\n",
        "            print(f\"Epoch {epoch}: Average Loss = {total_loss / len(train_dataset)}\")\n",
        "\n",
        "# 3. DPO Implementation\n",
        "\n",
        "def prepare_dpo_dataset(dataset):\n",
        "    \"\"\"Prepare dataset for DPO training\"\"\"\n",
        "    return {\n",
        "        \"prompt\": dataset[\"prompt\"],\n",
        "        \"chosen\": dataset[\"chosen_response\"],\n",
        "        \"rejected\": dataset[\"rejected_response\"]\n",
        "    }\n",
        "\n",
        "def train_dpo(\n",
        "    model,\n",
        "    ref_model,\n",
        "    tokenizer,\n",
        "    dataset,\n",
        "    output_dir=\"./dpo_results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4\n",
        "):\n",
        "    \"\"\"Train model using DPO\"\"\"\n",
        "\n",
        "    # Prepare DPO training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.001,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    # Initialize DPO trainer\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model=model,\n",
        "        ref_model=ref_model,\n",
        "        args=training_args,\n",
        "        beta=0.1,  # Temperature parameter for DPO loss\n",
        "        train_dataset=dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512,\n",
        "        max_prompt_length=128\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    dpo_trainer.train()\n",
        "\n",
        "# 4. Example Usage\n",
        "\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"tinyllama-rlhf\")\n",
        "\n",
        "    # Setup models\n",
        "    model, tokenizer = setup_model_and_tokenizer()\n",
        "    ref_model, _ = setup_model_and_tokenizer()  # Reference model\n",
        "\n",
        "    # Prepare LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Example dataset (modify according to your data)\n",
        "    dataset = load_dataset(\"anthropic/hh-rlhf\")  # Example RLHF dataset\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.001,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\"\n",
        "    )\n",
        "\n",
        "    # For ORPO training\n",
        "    orpo_config = ORPOConfig()\n",
        "    orpo_trainer = ORPOTrainer(\n",
        "        model=model,\n",
        "        ref_model=ref_model,\n",
        "        tokenizer=tokenizer,\n",
        "        reward_model=None,  # Add your reward model here\n",
        "        config=orpo_config,\n",
        "        training_args=training_args\n",
        "    )\n",
        "\n",
        "    # For DPO training\n",
        "    dpo_dataset = prepare_dpo_dataset(dataset)\n",
        "    train_dpo(\n",
        "        model=model,\n",
        "        ref_model=ref_model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset=dpo_dataset\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# 5. Example Dataset Format\n",
        "\n",
        "\"\"\"\n",
        "Example dataset format for preference data:\n",
        "\n",
        "{\n",
        "    \"prompt\": \"What is the capital of France?\",\n",
        "    \"chosen_response\": \"The capital of France is Paris.\",\n",
        "    \"rejected_response\": \"The capital of France is London.\"\n",
        "}\n",
        "\n",
        "For ORPO, you'll also need a reward model that can score responses.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iYrrdTWfqjP_",
        "outputId": "3ffbf622-ac2b-413f-ea02-a2c9aa17138e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a01adb8281db>\u001b[0m in \u001b[0;36m<cell line: 237>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;31m# 5. Example Dataset Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-a01adb8281db>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# Initialize wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tinyllama-rlhf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Setup models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unreachable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    306\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"anonymous\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"force\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             directive = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    254\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;34mf\"You can find your API key in your browser here: {app_url}/authorize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0z2uJXOqvni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **e.) continued fine tuning**"
      ],
      "metadata": {
        "id": "c5yx8aFJq9H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required installations\n",
        "!pip install transformers datasets accelerate bitsandbytes peft wandb\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "import wandb\n",
        "from typing import Optional, Dict\n",
        "import json\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"Handles checkpoint loading, saving, and merging operations\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_id: str,\n",
        "        checkpoint_path: Optional[str] = None,\n",
        "        adapter_path: Optional[str] = None\n",
        "    ):\n",
        "        self.base_model_id = base_model_id\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.adapter_path = adapter_path\n",
        "\n",
        "    def setup_quantization(self):\n",
        "        \"\"\"Setup 4-bit quantization configuration\"\"\"\n",
        "        return BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=False\n",
        "        )\n",
        "\n",
        "    def load_base_model(self):\n",
        "        \"\"\"Load the base model with quantization\"\"\"\n",
        "        print(f\"Loading base model: {self.base_model_id}\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.base_model_id,\n",
        "            quantization_config=self.setup_quantization(),\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.base_model_id,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"Load model from checkpoint\"\"\"\n",
        "        print(f\"Loading checkpoint: {self.checkpoint_path}\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.checkpoint_path,\n",
        "            quantization_config=self.setup_quantization(),\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.checkpoint_path,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def load_and_merge_lora(self, model):\n",
        "        \"\"\"Load and merge LoRA adapter with the model\"\"\"\n",
        "        print(f\"Loading and merging LoRA adapter: {self.adapter_path}\")\n",
        "\n",
        "        model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            self.adapter_path\n",
        "        )\n",
        "\n",
        "        # Merge LoRA weights with base model\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_checkpoint(self, model, tokenizer, output_dir: str):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        print(f\"Saving checkpoint to: {output_dir}\")\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save model and tokenizer\n",
        "        model.save_pretrained(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "class ContinuedTrainer:\n",
        "    \"\"\"Handles continued fine-tuning process\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        training_args: TrainingArguments,\n",
        "        dataset_config: Dict\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.training_args = training_args\n",
        "        self.dataset_config = dataset_config\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        \"\"\"Prepare dataset for continued training\"\"\"\n",
        "        if self.dataset_config.get(\"path\"):\n",
        "            # Load from local file\n",
        "            if self.dataset_config[\"path\"].endswith('.json'):\n",
        "                with open(self.dataset_config[\"path\"], 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                dataset = Dataset.from_dict(data)\n",
        "            else:\n",
        "                dataset = load_dataset(\n",
        "                    \"json\",\n",
        "                    data_files=self.dataset_config[\"path\"]\n",
        "                )[\"train\"]\n",
        "        else:\n",
        "            # Load from Hugging Face hub\n",
        "            dataset = load_dataset(\n",
        "                self.dataset_config[\"name\"],\n",
        "                split=\"train\"\n",
        "            )\n",
        "\n",
        "        # Apply preprocessing\n",
        "        def preprocess(examples):\n",
        "            # Apply custom template if provided\n",
        "            if \"template\" in self.dataset_config:\n",
        "                texts = [\n",
        "                    self.dataset_config[\"template\"].format(**example)\n",
        "                    for example in examples\n",
        "                ]\n",
        "            else:\n",
        "                texts = examples[self.dataset_config[\"text_column\"]]\n",
        "\n",
        "            # Tokenize\n",
        "            tokenized = self.tokenizer(\n",
        "                texts,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.dataset_config.get(\"max_length\", 512)\n",
        "            )\n",
        "\n",
        "            return tokenized\n",
        "\n",
        "        processed_dataset = dataset.map(\n",
        "            preprocess,\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        return processed_dataset\n",
        "\n",
        "    def setup_lora(self):\n",
        "        \"\"\"Setup LoRA for continued training\"\"\"\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Execute training\"\"\"\n",
        "        # Prepare dataset\n",
        "        dataset = self.prepare_dataset()\n",
        "\n",
        "        # Setup data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=dataset,\n",
        "            data_collator=data_collator\n",
        "        )\n",
        "\n",
        "        # Start training\n",
        "        trainer.train()\n",
        "\n",
        "        return trainer\n",
        "\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"continued-finetuning\")\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"base_model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        \"checkpoint_path\": \"path/to/your/checkpoint\",  # Optional\n",
        "        \"adapter_path\": \"path/to/your/lora\",  # Optional\n",
        "        \"output_dir\": \"./continued_training\",\n",
        "        \"dataset_config\": {\n",
        "            \"path\": \"path/to/your/data.json\",  # Or use name from HF hub\n",
        "            \"text_column\": \"text\",\n",
        "            \"max_length\": 512,\n",
        "            \"template\": \"{instruction}\\n{input}\\n{output}\"  # Optional\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Initialize checkpoint manager\n",
        "    checkpoint_manager = CheckpointManager(\n",
        "        base_model_id=config[\"base_model_id\"],\n",
        "        checkpoint_path=config.get(\"checkpoint_path\"),\n",
        "        adapter_path=config.get(\"adapter_path\")\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    if config.get(\"checkpoint_path\"):\n",
        "        model, tokenizer = checkpoint_manager.load_checkpoint()\n",
        "    else:\n",
        "        model, tokenizer = checkpoint_manager.load_base_model()\n",
        "\n",
        "    # Merge LoRA if provided\n",
        "    if config.get(\"adapter_path\"):\n",
        "        model = checkpoint_manager.load_and_merge_lora(model)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.001,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"no\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_steps=100\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = ContinuedTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        training_args=training_args,\n",
        "        dataset_config=config[\"dataset_config\"]\n",
        "    )\n",
        "\n",
        "    # Setup LoRA for continued training\n",
        "    model = trainer.setup_lora()\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save final checkpoint\n",
        "    checkpoint_manager.save_checkpoint(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        output_dir=config[\"output_dir\"]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\"\"\"\n",
        "Example JSON data format:\n",
        "{\n",
        "    \"data\": [\n",
        "        {\n",
        "            \"instruction\": \"Translate the following English text to French\",\n",
        "            \"input\": \"Hello, how are you?\",\n",
        "            \"output\": \"Bonjour, comment allez-vous?\"\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "36-IrLFHq_3H",
        "outputId": "cd7af87a-4069-4af9-b972-c51e35c7b4d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2e03c9fb40a8>\u001b[0m in \u001b[0;36m<cell line: 286>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \"\"\"\n",
            "\u001b[0;32m<ipython-input-3-2e03c9fb40a8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# Initialize wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"continued-finetuning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unreachable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    306\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"anonymous\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"force\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             directive = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    254\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;34mf\"You can find your API key in your browser here: {app_url}/authorize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             )\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **f.) finetune unsloth for mental health development chatbot**"
      ],
      "metadata": {
        "id": "yrS3jkJdrMAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required installations\n",
        "!pip install -q unsloth[cu121] torch==2.1.2 transformers==4.37.2 accelerate==0.26.1 bitsandbytes==0.42.0\n",
        "!pip install datasets wandb trl\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from unsloth import FastLanguageModel\n",
        "import pandas as pd\n",
        "import json\n",
        "from typing import Dict, List, Optional\n",
        "import wandb\n",
        "from datetime import datetime\n",
        "\n",
        "class MentalHealthDataProcessor:\n",
        "    \"\"\"Handles mental health dataset preparation and processing\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_length: int = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Define conversation templates\n",
        "        self.templates = {\n",
        "            \"basic\": (\n",
        "                \"<|system|>You are a supportive AI assistant trained to provide \"\n",
        "                \"mental health information and emotional support. Always encourage \"\n",
        "                \"professional help for serious concerns.</s>\\n\"\n",
        "                \"<|user|>{user_input}</s>\\n\"\n",
        "                \"<|assistant|>{assistant_response}</s>\"\n",
        "            ),\n",
        "            \"therapeutic\": (\n",
        "                \"<|system|>You are a supportive AI assistant. Focus on empathy, \"\n",
        "                \"validation, and encouraging professional help when needed. Never \"\n",
        "                \"provide medical advice or diagnosis.</s>\\n\"\n",
        "                \"<|user|>{user_input}</s>\\n\"\n",
        "                \"<|assistant|>{assistant_response}</s>\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def load_custom_dataset(self, file_path: str) -> Dataset:\n",
        "        \"\"\"Load dataset from JSON file\"\"\"\n",
        "        if file_path.endswith('.json'):\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "        elif file_path.endswith('.csv'):\n",
        "            data = pd.read_csv(file_path).to_dict('records')\n",
        "\n",
        "        return Dataset.from_dict({\n",
        "            'conversations': data\n",
        "        })\n",
        "\n",
        "    def format_conversation(self, example: Dict, template_type: str = \"basic\") -> str:\n",
        "        \"\"\"Format conversation using template\"\"\"\n",
        "        template = self.templates[template_type]\n",
        "        return template.format(\n",
        "            user_input=example['user_input'],\n",
        "            assistant_response=example['assistant_response']\n",
        "        )\n",
        "\n",
        "    def preprocess_dataset(self, dataset: Dataset, template_type: str = \"basic\"):\n",
        "        \"\"\"Preprocess and tokenize dataset\"\"\"\n",
        "\n",
        "        def process_example(example):\n",
        "            # Format conversation\n",
        "            text = self.format_conversation(example, template_type)\n",
        "\n",
        "            # Tokenize\n",
        "            encodings = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": encodings[\"input_ids\"][0],\n",
        "                \"attention_mask\": encodings[\"attention_mask\"][0],\n",
        "                \"labels\": encodings[\"input_ids\"][0].clone()\n",
        "            }\n",
        "\n",
        "        return dataset.map(process_example, remove_columns=dataset.column_names)\n",
        "\n",
        "class MentalHealthModelTrainer:\n",
        "    \"\"\"Handles model training for mental health chatbot\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_id: str,\n",
        "        output_dir: str,\n",
        "        max_seq_length: int = 512,\n",
        "        load_in_4bit: bool = True\n",
        "    ):\n",
        "        self.base_model_id = base_model_id\n",
        "        self.output_dir = output_dir\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.load_in_4bit = load_in_4bit\n",
        "\n",
        "        # Initialize model and tokenizer\n",
        "        self.setup_model_and_tokenizer()\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Initialize model and tokenizer using Unsloth\"\"\"\n",
        "        print(f\"Loading model: {self.base_model_id}\")\n",
        "\n",
        "        # Load model and tokenizer using Unsloth\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.base_model_id,\n",
        "            max_seq_length=self.max_seq_length,\n",
        "            load_in_4bit=self.load_in_4bit,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Add special tokens for chat format\n",
        "        special_tokens = {\n",
        "            \"pad_token\": \"<|pad|>\",\n",
        "            \"sep_token\": \"</s>\",\n",
        "            \"additional_special_tokens\": [\"<|system|>\", \"<|user|>\", \"<|assistant|>\"]\n",
        "        }\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def prepare_training_args(\n",
        "        self,\n",
        "        num_train_epochs: int = 3,\n",
        "        per_device_train_batch_size: int = 4,\n",
        "        gradient_accumulation_steps: int = 4\n",
        "    ):\n",
        "        \"\"\"Prepare training arguments\"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            num_train_epochs=num_train_epochs,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            learning_rate=2e-5,\n",
        "            weight_decay=0.01,\n",
        "            logging_steps=10,\n",
        "            save_strategy=\"epoch\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            report_to=\"wandb\",\n",
        "            remove_unused_columns=False,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            warmup_steps=100,\n",
        "            optim=\"adamw_torch\",\n",
        "            fp16=True,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"loss\"\n",
        "        )\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_dataset: Dataset,\n",
        "        val_dataset: Optional[Dataset] = None,\n",
        "        training_args: Optional[TrainingArguments] = None\n",
        "    ):\n",
        "        \"\"\"Execute training\"\"\"\n",
        "        if training_args is None:\n",
        "            training_args = self.prepare_training_args()\n",
        "\n",
        "        # Configure Unsloth training\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=16,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.05,\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        self.model.train()\n",
        "\n",
        "        trainer = self.model.get_trainer(\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            args=training_args,\n",
        "            max_seq_length=self.max_seq_length,\n",
        "            packing=True\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        return trainer\n",
        "\n",
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"mental-health-chatbot\")\n",
        "\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"base_model_id\": \"microsoft/phi-2\",  # or another suitable base model\n",
        "        \"output_dir\": \"./mental_health_bot\",\n",
        "        \"dataset_path\": \"path/to/your/mental_health_data.json\",\n",
        "        \"max_seq_length\": 512,\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"batch_size\": 4\n",
        "    }\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = MentalHealthModelTrainer(\n",
        "        base_model_id=config[\"base_model_id\"],\n",
        "        output_dir=config[\"output_dir\"],\n",
        "        max_seq_length=config[\"max_seq_length\"]\n",
        "    )\n",
        "\n",
        "    # Initialize data processor\n",
        "    data_processor = MentalHealthDataProcessor(\n",
        "        tokenizer=trainer.tokenizer,\n",
        "        max_length=config[\"max_seq_length\"]\n",
        "    )\n",
        "\n",
        "    # Load and process dataset\n",
        "    dataset = data_processor.load_custom_dataset(config[\"dataset_path\"])\n",
        "    processed_dataset = data_processor.preprocess_dataset(dataset, template_type=\"therapeutic\")\n",
        "\n",
        "    # Split dataset\n",
        "    train_val = processed_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = trainer.prepare_training_args(\n",
        "        num_train_epochs=config[\"num_train_epochs\"],\n",
        "        per_device_train_batch_size=config[\"batch_size\"]\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trainer.train(\n",
        "        train_dataset=train_val[\"train\"],\n",
        "        val_dataset=train_val[\"test\"],\n",
        "        training_args=training_args\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    trainer.model.save_pretrained(config[\"output_dir\"])\n",
        "    trainer.tokenizer.save_pretrained(config[\"output_dir\"])\n",
        "\n",
        "\"\"\"\n",
        "Example dataset format:\n",
        "{\n",
        "    \"conversations\": [\n",
        "        {\n",
        "            \"user_input\": \"I've been feeling really anxious lately and having trouble sleeping.\",\n",
        "            \"assistant_response\": \"I hear how difficult this has been for you. Anxiety and sleep problems can be really challenging. While I can share some general relaxation techniques, it's important to know that a mental health professional can provide personalized strategies. Would you like to talk about some simple calming techniques that others have found helpful?\"\n",
        "        },\n",
        "        {\n",
        "            \"user_input\": \"Sometimes I feel overwhelmed by everything I need to do.\",\n",
        "            \"assistant_response\": \"It's completely normal to feel overwhelmed sometimes. Let's break this down together. Could you tell me more about what's on your plate right now? We can explore some manageable steps while remembering that it's also okay to reach out to a counselor or therapist for additional support.\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Hz9q9QxrNcz",
        "outputId": "693e23b5-cbd3-4d27-d7f7-68d9a11ab155"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Cannot install accelerate==0.26.1, torch==2.1.2 and unsloth because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (0.34.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.3)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.46.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.0+cu121)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'unsloth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-17273a099b05>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **g.) use unsloth to finetune a model and export to ollama and show inference**"
      ],
      "metadata": {
        "id": "WIttylz4rN3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q unsloth accelerate transformers datasets bitsandbytes\n",
        "!curl https://ollama.ai/install.sh | sh  # Install Ollama\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "from typing import Dict, List, Optional\n",
        "import requests\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Handles model training and export to Ollama\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model_id: str,\n",
        "        output_dir: str,\n",
        "        max_seq_length: int = 512,\n",
        "        load_in_4bit: bool = True\n",
        "    ):\n",
        "        self.base_model_id = base_model_id\n",
        "        self.output_dir = output_dir\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.load_in_4bit = load_in_4bit\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize model and tokenizer\n",
        "        self.setup_model_and_tokenizer()\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Initialize model and tokenizer using Unsloth\"\"\"\n",
        "        print(f\"Loading model: {self.base_model_id}\")\n",
        "\n",
        "        # Load model and tokenizer using Unsloth\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.base_model_id,\n",
        "            max_seq_length=self.max_seq_length,\n",
        "            load_in_4bit=self.load_in_4bit,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Add special tokens if needed\n",
        "        special_tokens = {\n",
        "            \"pad_token\": \"<|pad|>\",\n",
        "            \"eos_token\": \"</s>\",\n",
        "            \"bos_token\": \"<s>\"\n",
        "        }\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def prepare_dataset(self, dataset_path: str):\n",
        "        \"\"\"Prepare dataset for training\"\"\"\n",
        "        if dataset_path.endswith('.json'):\n",
        "            with open(dataset_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            dataset = load_dataset('json', data_files=dataset_path)\n",
        "        else:\n",
        "            dataset = load_dataset(dataset_path)\n",
        "\n",
        "        def preprocess(examples):\n",
        "            # Assuming format: instruction, input, output\n",
        "            prompts = [\n",
        "                f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
        "                for instruction, input_text, output in zip(\n",
        "                    examples['instruction'],\n",
        "                    examples['input'],\n",
        "                    examples['output']\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            return self.tokenizer(\n",
        "                prompts,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_seq_length\n",
        "            )\n",
        "\n",
        "        processed_dataset = dataset.map(\n",
        "            preprocess,\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        return processed_dataset\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_dataset,\n",
        "        num_train_epochs: int = 3,\n",
        "        per_device_train_batch_size: int = 4,\n",
        "        gradient_accumulation_steps: int = 4\n",
        "    ):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        # Configure Unsloth training\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=16,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0.05,\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = {\n",
        "            \"num_train_epochs\": num_train_epochs,\n",
        "            \"per_device_train_batch_size\": per_device_train_batch_size,\n",
        "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "            \"learning_rate\": 2e-5,\n",
        "            \"output_dir\": self.output_dir,\n",
        "            \"save_steps\": 100\n",
        "        }\n",
        "\n",
        "        # Train the model\n",
        "        trainer = self.model.get_trainer(\n",
        "            train_dataset=train_dataset,\n",
        "            args=training_args,\n",
        "            max_seq_length=self.max_seq_length,\n",
        "            packing=True\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        return trainer\n",
        "\n",
        "    def export_to_gguf(self):\n",
        "        \"\"\"Export model to GGUF format\"\"\"\n",
        "        # Save model in Hugging Face format first\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "\n",
        "        # Convert to GGUF using llama.cpp\n",
        "        gguf_output = os.path.join(self.output_dir, \"model.gguf\")\n",
        "        command = (\n",
        "            f\"python -m llama_cpp.convert_hf_to_gguf \"\n",
        "            f\"--outfile {gguf_output} \"\n",
        "            f\"--input {self.output_dir}\"\n",
        "        )\n",
        "        subprocess.run(command, shell=True, check=True)\n",
        "\n",
        "        return gguf_output\n",
        "\n",
        "    def export_to_ollama(self, model_name: str, api_base: str = \"http://localhost:11434\"):\n",
        "        \"\"\"Export model to Ollama\"\"\"\n",
        "        gguf_path = self.export_to_gguf()\n",
        "\n",
        "        # Upload model to Ollama\n",
        "        url = f\"{api_base}/api/upload\"\n",
        "        files = {\"model\": open(gguf_path, \"rb\")}\n",
        "        response = requests.post(url, files=files)\n",
        "        response.raise_for_status()\n",
        "        model_id = response.json()[\"model_id\"]\n",
        "\n",
        "        # Update model configuration\n",
        "        url = f\"{api_base}/api/v1/push\"\n",
        "        url = f\"{api_base}/api/v1/push\"\n",
        "        url = f\"{api_base}/api/v1/push\"\n",
        "        payload = {\n",
        "            \"name\": model_name,\n",
        "            \"base_model\": model_id,\n",
        "            \"format\": \"ggml\"\n",
        "        }\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()\n",
        "        print(\"Model configuration updated successfully.\")\n",
        "        return model_id\n",
        "\n",
        "\n",
        "class OllamaInference:\n",
        "    \"\"\"Handles inference with Ollama\"\"\"\n",
        "    def __init__(self, model_name: str, api_base: str = \"http://localhost:11434\"):\n",
        "        self.model_name = model_name\n",
        "        self.api_base = api_base\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        system: str = \"\",\n",
        "        temperature: float = 0.7,\n",
        "        max_tokens: int = 500\n",
        "    ) -> str:\n",
        "        \"\"\"Generate response using Ollama API\"\"\"\n",
        "        url = f\"{self.api_base}/api/generate\"\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"system\": system,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Collect streaming response\n",
        "        full_response = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                json_response = json.loads(line)\n",
        "                full_response += json_response.get(\"response\", \"\")\n",
        "\n",
        "                if json_response.get(\"done\", False):\n",
        "                    break\n",
        "\n",
        "        return full_response.strip()\n",
        "\n",
        "    def chat(\n",
        "        self,\n",
        "        messages: List[Dict[str, str]],\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Chat with the model using message format\"\"\"\n",
        "        url = f\"{self.api_base}/api/chat\"\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        return response.json()[\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        \"base_model_id\": \"microsoft/phi-2\",  # or another suitable base model\n",
        "        \"output_dir\": \"./exported_model\",\n",
        "        \"dataset_path\": \"path/to/your/dataset.json\",\n",
        "        \"ollama_model_name\": \"my-custom-model\"\n",
        "    }\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = ModelTrainer(\n",
        "        base_model_id=config[\"base_model_id\"],\n",
        "        output_dir=config[\"output_dir\"]\n",
        "    )\n",
        "\n",
        "    # Prepare dataset and train\n",
        "    dataset = trainer.prepare_dataset(config[\"dataset_path\"])\n",
        "    trainer.train(dataset)\n",
        "\n",
        "    # Export to Ollama\n",
        "    trainer.export_to_ollama(config[\"ollama_model_name\"])\n",
        "\n",
        "    # Initialize inference\n",
        "    inference = OllamaInference(config[\"ollama_model_name\"])\n",
        "\n",
        "    # Example usage\n",
        "    system_prompt = \"You are a helpful AI assistant.\"\n",
        "    user_prompt = \"What is the capital of France?\"\n",
        "\n",
        "    # Generation example\n",
        "    response = inference.generate(\n",
        "        prompt=user_prompt,\n",
        "        system=system_prompt\n",
        "    )\n",
        "    print(\"Generated Response:\", response)\n",
        "\n",
        "    # Chat example\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    chat_response = inference.chat(messages)\n",
        "    print(\"Chat Response:\", chat_response)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Example dataset format:\n",
        "\"\"\"\n",
        "{\n",
        "    \"data\": [\n",
        "        {\n",
        "            \"instruction\": \"Answer the following question\",\n",
        "            \"input\": \"What is the capital of France?\",\n",
        "            \"output\": \"The capital of France is Paris.\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Usage Guide:\n",
        "\"\"\"\n",
        "1. Start Ollama server:\n",
        "   ollama serve\n",
        "\n",
        "2. Run the script to train and export:\n",
        "   python script.py\n",
        "\n",
        "3. Run inference using either the Python API:\n",
        "   inference = OllamaInference(\"my-custom-model\")\n",
        "   response = inference.generate(\"What is the capital of France?\")\n",
        "\n",
        "4. Or use CLI:\n",
        "   ollama run my-custom-model \"What is the capital of France?\"\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DnO8oQTjrQlS",
        "outputId": "0cb51820-d861-4535-d824-8d5b1df5d60f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m847.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13320    0 13320    0     0  46348      0 --:--:-- --:--:-- --:--:-- 46411\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-228c88663a5f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Torch 2.4 has including_emulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mmajor_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0mSUPPORTS_BFLOAT16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmajor_version\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3jmaK-Nr80B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}