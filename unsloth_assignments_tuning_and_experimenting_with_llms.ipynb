{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praneeth-18/Unsloth---Tuning-and-experimenting-with-LLMs/blob/main/unsloth_assignments_tuning_and_experimenting_with_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDplPziqn6Uj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPt5jwjCG6PH",
        "outputId": "f05957f1-7ba2-4954-ac5a-38a497884927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate peft bitsandbytes\n",
        "!pip install -q torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TinyLlama**"
      ],
      "metadata": {
        "id": "PcTN20ttHPTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_tinyllama():\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Print available GPU memory\n",
        "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Load a very small dataset\n",
        "    dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:10]\")\n",
        "\n",
        "    # Print dataset example to debug\n",
        "    print(\"\\nDataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data with proper dictionary access\n",
        "    def format_data(example):\n",
        "        try:\n",
        "            # Print example structure\n",
        "            print(\"\\nProcessing example:\")\n",
        "            print(example)\n",
        "\n",
        "            return f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error in format_data: {e}\")\n",
        "            print(f\"Example structure: {example}\")\n",
        "            raise e\n",
        "\n",
        "    # Process and tokenize dataset\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda examples: {\n",
        "            'input_ids': tokenizer(\n",
        "                format_data(examples),\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )['input_ids'],\n",
        "            'attention_mask': tokenizer(\n",
        "                format_data(examples),\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=None\n",
        "            )['attention_mask']\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # LoRA config\n",
        "    config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"tinyllama_results\",\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        save_steps=5,\n",
        "        logging_steps=5,\n",
        "        learning_rate=2e-4,\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=2,\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Save\n",
        "    model.save_pretrained(\"tinyllama_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_model(model, tokenizer, prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_length=200)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory first\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Run training with error handling\n",
        "try:\n",
        "    print(\"Starting training...\")\n",
        "    model, tokenizer = train_tinyllama()\n",
        "\n",
        "    print(\"\\nTesting model...\")\n",
        "    test_prompt = \"Explain what is machine learning in simple terms\"\n",
        "    result = test_model(model, tokenizer, test_prompt)\n",
        "    print(\"\\nTest Result:\")\n",
        "    print(result)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e66f532d30444a4ab790830890dcb74f",
            "0b6048d5c63a4a0eaf361cd14a5927fa",
            "8ea43fb9b1654c359d55975b9e64f364",
            "70c6dcd3e1a24835b0fb1c4bc3550e1b",
            "17827b6bc8fa4eacbc276173902abdea",
            "3f55d5f043ef43ea8d1a6074a44a0c42",
            "f8026330927945cf98ebb0aeb3992ca2",
            "8f01e884799b48589c0fd19dc82f6d66",
            "63cf1ab8ad9a48c7803aff94ee59cc92",
            "6ff184c2890d43c688f5133321954129",
            "fc24a91bb6864b398a5395d2218be854"
          ]
        },
        "id": "wicnxsJvHL5q",
        "outputId": "8f155b31-4f21-4adb-9798-7c78042c1518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "GPU memory available: 15.84 GB\n",
            "\n",
            "Dataset example:\n",
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e66f532d30444a4ab790830890dcb74f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing example:\n",
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'When did Virgin Australia start operating?', 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\", 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Which is a species of fish? Tope or Rope', 'context': '', 'response': 'Tope', 'category': 'classification'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Which is a species of fish? Tope or Rope', 'context': '', 'response': 'Tope', 'category': 'classification'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why can camels survive for long without water?', 'context': '', 'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why can camels survive for long without water?', 'context': '', 'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': \"Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\", 'context': '', 'response': 'The name of the third daughter is Alice', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': \"Alice's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\", 'context': '', 'response': 'The name of the third daughter is Alice', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'When was Tomoaki Komorida born?', 'context': 'Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.', 'response': 'Tomoaki Komorida was born on July 10,1981.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'When was Tomoaki Komorida born?', 'context': 'Komorida was born in Kumamoto Prefecture on July 10, 1981. After graduating from high school, he joined the J1 League club Avispa Fukuoka in 2000. Although he debuted as a midfielder in 2001, he did not play much and the club was relegated to the J2 League at the end of the 2001 season. In 2002, he moved to the J2 club Oita Trinita. He became a regular player as a defensive midfielder and the club won the championship in 2002 and was promoted in 2003. He played many matches until 2005. In September 2005, he moved to the J2 club Montedio Yamagata. In 2006, he moved to the J2 club Vissel Kobe. Although he became a regular player as a defensive midfielder, his gradually was played less during the summer. In 2007, he moved to the Japan Football League club Rosso Kumamoto (later Roasso Kumamoto) based in his local region. He played as a regular player and the club was promoted to J2 in 2008. Although he did not play as much, he still played in many matches. In 2010, he moved to Indonesia and joined Persela Lamongan. In July 2010, he returned to Japan and joined the J2 club Giravanz Kitakyushu. He played often as a defensive midfielder and center back until 2012 when he retired.', 'response': 'Tomoaki Komorida was born on July 10,1981.', 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'If I have more pieces at the time of stalemate, have I won?', 'context': 'Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\\n\\nThe outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.', 'response': \"No. \\nStalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position\", 'category': 'information_extraction'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'If I have more pieces at the time of stalemate, have I won?', 'context': 'Stalemate is a situation in chess where the player whose turn it is to move is not in check and has no legal move. Stalemate results in a draw. During the endgame, stalemate is a resource that can enable the player with the inferior position to draw the game rather than lose. In more complex positions, stalemate is much rarer, usually taking the form of a swindle that succeeds only if the superior side is inattentive.[citation needed] Stalemate is also a common theme in endgame studies and other chess problems.\\n\\nThe outcome of a stalemate was standardized as a draw in the 19th century. Before this standardization, its treatment varied widely, including being deemed a win for the stalemating player, a half-win for that player, or a loss for that player; not being permitted; and resulting in the stalemated player missing a turn. Stalemate rules vary in other games of the chess family.', 'response': \"No. \\nStalemate is a drawn position. It doesn't matter who has captured more pieces or is in a winning position\", 'category': 'information_extraction'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Given a reference text about Lollapalooza, where does it take place, who started it and what is it?', 'context': \"Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\\n\\nLollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.\", 'response': \"Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.\", 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Given a reference text about Lollapalooza, where does it take place, who started it and what is it?', 'context': \"Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\\n\\nLollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.\", 'response': \"Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.\", 'category': 'closed_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who gave the UN the land in NY to build their HQ', 'context': '', 'response': 'John D Rockerfeller', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who gave the UN the land in NY to build their HQ', 'context': '', 'response': 'John D Rockerfeller', 'category': 'open_qa'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why mobile is bad for human', 'context': '', 'response': 'We are always engaged one phone which is not good.', 'category': 'brainstorming'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Why mobile is bad for human', 'context': '', 'response': 'We are always engaged one phone which is not good.', 'category': 'brainstorming'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who was John Moses Browning?', 'context': \"John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\\n\\nBrowning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\\n\\nBrowning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.\", 'response': \"John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at the age of 13, and was awarded his first patent when he was 24.\\n\\nHe  designed the first reliable automatic pistol, and the first gas-operated firearm, as well inventing or improving single-shot, lever-action, and pump-action rifles and shotguns.\\n\\nToday, he is most well-known for the M1911 pistol, the Browning Automatic Rifle, and the Auto-5 shotgun, all of which are in still in current production in either their original design, or with minor changes.  His M1911 and Hi-Power pistols designs are some of the most reproduced firearms in the world today.\", 'category': 'information_extraction'}\n",
            "\n",
            "Processing example:\n",
            "{'instruction': 'Who was John Moses Browning?', 'context': \"John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\\n\\nBrowning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\\n\\nBrowning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.\", 'response': \"John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at the age of 13, and was awarded his first patent when he was 24.\\n\\nHe  designed the first reliable automatic pistol, and the first gas-operated firearm, as well inventing or improving single-shot, lever-action, and pump-action rifles and shotguns.\\n\\nToday, he is most well-known for the M1911 pistol, the Browning Automatic Rifle, and the Auto-5 shotgun, all of which are in still in current production in either their original design, or with minor changes.  His M1911 and Hi-Power pistols designs are some of the most reproduced firearms in the world today.\", 'category': 'information_extraction'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-10-118a6cd8fc56>\", line 121, in <cell line: 119>\n",
            "    model, tokenizer = train_tinyllama()\n",
            "  File \"<ipython-input-10-118a6cd8fc56>\", line 102, in train_tinyllama\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phi**"
      ],
      "metadata": {
        "id": "kljsz9IVHSme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_phi():\n",
        "    print(\"Starting Phi training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Phi model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/phi-1_5\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"microsoft/phi-1_5\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"instruction\": \"Write a Python function to add two numbers\",\n",
        "            \"response\": \"def add_numbers(a, b):\\n    return a + b\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Create a function to check if a number is prime\",\n",
        "            \"response\": \"def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n ** 0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"Write a function to reverse a string\",\n",
        "            \"response\": \"def reverse_string(s):\\n    return s[::-1]\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data\n",
        "    def format_data(example):\n",
        "        return f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    def tokenize_function(examples):\n",
        "        formatted_text = format_data(examples)\n",
        "        return tokenizer(\n",
        "            formatted_text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./phi_results\",\n",
        "        num_train_epochs=3,           # Train for a few epochs\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,              # Log every step\n",
        "        save_steps=3,                 # Save every 3 steps\n",
        "        max_steps=10                  # Only train for 10 steps\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./phi_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_phi(model, tokenizer, prompt):\n",
        "    print(f\"\\nTesting with prompt: {prompt}\")\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nStarting Phi training process...\")\n",
        "    model, tokenizer = train_phi()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_prompts = [\n",
        "        \"Write a Python function to calculate factorial\",\n",
        "        \"Create a function to find the maximum number in a list\"\n",
        "    ]\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        result = test_phi(model, tokenizer, prompt)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Result: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./phi_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ff01fd1e1e14eb8b7ad52f81a1ea3fd",
            "f318fd2b55704b75a633f556c871cd89",
            "5092a2f9bf9a4057bca69edf8007a437",
            "038addf1d7d24f889d32602b8b70526c",
            "8f387f54effd4b3498300df69021125c",
            "b413fe8342774b59a15c01e3dfdf93d0",
            "e0d0fa30bb4c4552bb862dcab154a083",
            "3eef26c3a696472382963211beccd6d1",
            "6c86fde7f35242208271ce9327898b25",
            "67efdf77bc8c43328b31743ef19f8a6e",
            "5dbbe92e8e384538a1edbf509ce3a67f",
            "26ff7fac82334328ad8e9139b81d4d21",
            "edb562214546474f81a017dfeb53857a",
            "7582766f76404a6da354563c1f4d4f9f",
            "cf255189aa2e41e9917e34fd0c169786",
            "01745cf39005455d800e5dcfc46be778",
            "e053161504cd46b8be423010fcc27a64",
            "3c7538ab80234c4d8cdafca327cdcffa",
            "3212c12dac5f4e32aa82d853b4c8fd2f",
            "22076b624b964bedb1b0180e690be2e6",
            "e4d5d88eff954ad5b20fbbb0b1f00d68",
            "afb99e89c44e49a084ff7f71026adfc7",
            "9444ec8101604d348a6b3bf3e7b68aa0",
            "dc0921f1619449ecb9bf156586e83699",
            "0d4371ad347848bca25caf9c5772db32",
            "54f1dd0cd6be482682f83be336482bdb",
            "053a5e9675de4eb0989e8eedf9a8900c",
            "e107f384157b4a3284e610c28b206369",
            "5c4daf55be9c41b8b322d54ae1d06b76",
            "fc934732aa0c45789225381f32c7c32b",
            "09a18e032ae14fbba47bf1bb3021845c",
            "6ddc3e3894b547ab8931c227b6e469cd",
            "c81fef8ee05d4f8fb9d15f85b72da69d",
            "840d19dabe4b45e692253b0d3853451b",
            "9137b435bc9849799f67ddcdc6d42591",
            "61e1b52c19fc4b29868f6e498aaef71e",
            "bc715a33cf6645ceb20cdb7bf0057015",
            "db598876dcc54d778448ed124812308e",
            "fcde8f30b1544180b94ea7d2524b727e",
            "5ceb52a8065d499c868c8385a92ef4f0",
            "7c2dcce21e634d2f9380383d28123b6d",
            "fef9e0b2dbbc454a9d9c5a0ff63d94b0",
            "2998e28451dd4f25ad6b13e4eb93e079",
            "364196c7b36c45fc8e7bf5ffa96b33c8",
            "670e8db05f924cff8585fa10fea01d7d",
            "8fdaebd5faa04eff98f18f0dd0ed6c6e",
            "e955c0cc2ec04ea1bac850e5d4008e76",
            "c6117cb42b484489af2e9cfac4f50439",
            "dd7fba37894b453b99cd778ee83830e3",
            "f83a0d67edcc49278bab7c08253afaaa",
            "4de18af75f6045e28d1443126db45296",
            "62c7a2fd5eac4904ab181a86e44cf7ce",
            "17de9c0fa1d14c4a9ea826aa53ab4472",
            "ed1329222fba4fd991a2fd58452bcf97",
            "2b126a1abfc242dc8122aac0ea6489e9",
            "4da67081204d4cf49fc60276990763b4",
            "4735c288570e42d090a7e45be4017d7b",
            "4af0e3be9aa24f33b01008b62ce1c076",
            "628b59b0b8bb4544998ab983748c941d",
            "6b4ac53c97684a6eb0e934cfe8e4b56f",
            "6e9ddb81447e4f4cb43e9c0e9a125065",
            "4dad24d9376b4fc4b13103a1513a5468",
            "c8dd55165e67446a877948da9ed0c940",
            "513344a9be304eccaf09aef42b907ea0",
            "6b4366e195984b36ad70df3c0e35cf29",
            "efb529e28b204d01ab5772b4637c8c5e",
            "8b6319ed3cbb4dc9a123989cc4759231",
            "f68187a41ef44fb183f67baf1130edca",
            "c789865a92104b7caea4c20995ae7295",
            "df8cd63f56eb49f59bc38b188b8052f4",
            "61b005cc4e1e450484a47e2e9be42858",
            "bd7c367370cf49e2ad34299f69f79b0d",
            "ca706de209724e10acecc1291dffe815",
            "f580fb876e894154a10918cab47f0137",
            "999e1e39c1194cb6971284350aaec16d",
            "20c8d40eef564e0c8f23e332a968501a",
            "d32f06da56e548928818a18cd820b58e",
            "53933d2b1fa348f4b8d01dac412dd646",
            "904b022182cc403990b16fc00c37f113",
            "e2f006fcaedf498a9ec249f02ae52f39",
            "ad12c9cfb3d84efda474bc6504849e05",
            "cfef8cb8e79c48faafb95471cc4837bb",
            "ce66de6833d943aa8074687859873c62",
            "74577a4e5a63479398886394bde72861",
            "825c7a6102b348f9baa3215c0b9a503f",
            "6ceaa9646c984db9b409fe5c696bc3c8",
            "b023a59e6f0c45d5b33912cb42e8e7dd",
            "5592d75806464f378cc46b71b0a26335",
            "b71036d90e8f4e87a6f6b33831256827",
            "30c9643f694842d99e92d98f7cba9c80",
            "af888b4d37d740a688f8f04ea0f60f7e",
            "efceec0674964afb8e3fb603fc07ff4e",
            "9fd432025d544634b0d899e30f6ccba7",
            "5f3969cf212341bfbdfcf48b353ca165",
            "183eaeb871dd43538b3c23b9aafd8897",
            "be357be09166444fabb2cee8591ab334",
            "d971eed6c8d14f14bd578d51862b2725",
            "3f99cabdbd364ea78d1a481e147a5c42",
            "bc848dd6b50f4aea8d95c813402aa616",
            "94f6ff117f004d7f9b212439ebd467ac",
            "337882d306b94ac5a0132bc6c7866898",
            "d4e7573f22934d41a9b6d24f8d2fcdf4",
            "9d38eb288ae045f8af0bbf14441a7720",
            "0be322df609e4b0eacb2a92d9f519d37",
            "01b70c5a3be64bc2bcd1222a72f4efd4",
            "5f9f2af7074c4e338ed9f87d2aef67fc",
            "d1dd3e9a48e5484cba2689d378d269ca",
            "2e3325e34e6848c48f297bdab3f96449",
            "71b2d9a03fe64c24b8cf94dfed5a79bd",
            "3245838e4f854511b7e618724adf4bab"
          ]
        },
        "id": "mqZJdi_dHTQ-",
        "outputId": "c7052e97-d138-4abf-ba34-4c969ca32314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing GPU memory...\n",
            "\n",
            "Starting Phi training process...\n",
            "Starting Phi training setup...\n",
            "Loading Phi model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ff01fd1e1e14eb8b7ad52f81a1ea3fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26ff7fac82334328ad8e9139b81d4d21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9444ec8101604d348a6b3bf3e7b68aa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "840d19dabe4b45e692253b0d3853451b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "670e8db05f924cff8585fa10fea01d7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4da67081204d4cf49fc60276990763b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b6319ed3cbb4dc9a123989cc4759231"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53933d2b1fa348f4b8d01dac412dd646"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b71036d90e8f4e87a6f6b33831256827"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Dataset example:\n",
            "{'instruction': 'Write a Python function to add two numbers', 'response': 'def add_numbers(a, b):\\n    return a + b'}\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94f6ff117f004d7f9b212439ebd467ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LoRA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-11-8a5ea4b83429>\", line 144, in <cell line: 142>\n",
            "    model, tokenizer = train_phi()\n",
            "  File \"<ipython-input-11-8a5ea4b83429>\", line 117, in train_phi\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gemma**"
      ],
      "metadata": {
        "id": "lM9XdfiwHVA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_gemma():\n",
        "    print(\"Starting Gemma training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Gemma model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"google/gemma-2b\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple QA dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"question\": \"What is machine learning?\",\n",
        "            \"answer\": \"Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance without being explicitly programmed.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How does photosynthesis work?\",\n",
        "            \"answer\": \"Photosynthesis is the process where plants convert sunlight, water, and carbon dioxide into glucose and oxygen, providing energy for the plant to grow.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What causes earthquakes?\",\n",
        "            \"answer\": \"Earthquakes are caused by the movement of tectonic plates beneath Earth's surface. When these plates move or collide, they release energy in the form of seismic waves.\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What is the theory of relativity?\",\n",
        "            \"answer\": \"The theory of relativity, developed by Einstein, describes how space and time are related and how mass and energy are equivalent (E=mc²).\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data\n",
        "    def format_data(example):\n",
        "        return f\"<start_of_turn>user\\nQuestion: {example['question']}<end_of_turn>\\n<start_of_turn>model\\nAnswer: {example['answer']}<end_of_turn>\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gemma_results\",\n",
        "        num_train_epochs=3,           # Train for a few epochs\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,              # Log every step\n",
        "        save_steps=3,                 # Save every 3 steps\n",
        "        max_steps=10                  # Only train for 10 steps\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./gemma_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_gemma(model, tokenizer, question):\n",
        "    print(f\"\\nTesting with question: {question}\")\n",
        "    prompt = f\"<start_of_turn>user\\nQuestion: {question}<end_of_turn>\\n<start_of_turn>model\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nStarting Gemma training process...\")\n",
        "    model, tokenizer = train_gemma()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_questions = [\n",
        "        \"What is the speed of light?\",\n",
        "        \"How does the human brain work?\",\n",
        "        \"What is quantum computing?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        result = test_gemma(model, tokenizer, question)\n",
        "        print(f\"\\nQuestion: {question}\")\n",
        "        print(f\"Answer: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./gemma_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Optional: Convert for Ollama\n",
        "def convert_for_ollama():\n",
        "    print(\"\\nConverting for Ollama...\")\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    %cd llama.cpp\n",
        "    !pip install -r requirements.txt\n",
        "    !python convert-lora-to-ggml.py ../gemma_adapter/adapter_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a144390a3580457abaaa9da712fcc9f3",
            "cbaa9194b152495a9d1033a436b63dc7",
            "ef0d493c93814ab597fc2fbebb265902",
            "49745d7c62514c458d007c23edc44950",
            "b4ec5c2e78044bacbc3030896f89bab7",
            "03f9717fbdd04d1ca532734f5ed309e7",
            "3af58b215cc9464c9627861e44a7f49f",
            "34d3e24d1edb4b0f84617b2267af1bb5",
            "a5fef0b232b3437c8646c0b875b7b55e",
            "b32e5cc96553449fbf7d380130ef71b7",
            "4d9f81546546455fbc573984ccc77dae",
            "74efaba1df474425b0ec8174b11df8e0",
            "81322108d6394c93bd9d500ad8ba9702",
            "5bf2c6002c9e465bb3d590b71acab85a",
            "242d137750dc47a1bc644c4a24fffa13",
            "d7b5e24a57d4457abfb361e66cf3b942",
            "0c7bc726c36f4fdfaa4f82fb3009f0d4",
            "d0427f5d2178417786b589b61dacb0bd",
            "3d95d184e2a445c0991531e9f556cf18",
            "db8307f8f27542499278980b43d8e914",
            "7a190779731d4d58911b4ab4a1fc47ac",
            "46477f5e49404d739b413fb28c4dc102",
            "10973e5f20fd4a0dade3fafbf07ba429",
            "3c9f988ac9444e0f9c5e5caaad87c82e",
            "6132c739663a4408b7753feb6c481641",
            "bdd5ccf252874f9b853aeb142abd7890",
            "138b871fc6d04f55af982427a873f345",
            "2a5234b0351c4ed595ce099c0c62dfa8",
            "b75be81addfc442abed11d83375c504d",
            "8b26abac542d474487773ea597a47964",
            "f476fb4738a34d4a93e275784fd630fe",
            "f7fb37564a5b4632aba021e8ec661d28",
            "767eff4aa2274f8a81bb2a1cf27507b3",
            "c1d5ab5e584244b7a3bc5dcc93c1aeba",
            "b2b42ac78c60470ebcad0ad032b50fb4",
            "fcfa9b30308d4ac18e4446ad1359b990",
            "7602823000a34cb1abfe82a1d31c220d",
            "5610f5fe7e9d417084be57d38663d12a",
            "b2c3c9db0f624473808baeadd7dbec67",
            "becfeedb90bb4a75ae6d579a5f4b2216",
            "39af044de00b4aef8b70b857404c43fe",
            "61412b46169c4341b61fc57eb394227b",
            "f46e0c3396504bfe9c5f7a6660d596cd",
            "fd27330b608844afbc0c3a64e61d94f2",
            "d1b6807d3f944d749df9a86cbdcb3684",
            "10d8ef11f1d04e87bc753ee3158b021a",
            "6b9146dcae1048108f2348cc51536d18",
            "d5a5750b88a641fb8080f38083b8e984",
            "a955a08ba64149c5ae365a28376d551c",
            "a1061e2de4b64e7cab9a2cbb403f54ac",
            "68fbe0958cda4832bc8d22bc836d14db",
            "d7bfe46da2e34ca5bbf54a14a1ad7b9f",
            "97b3f03554334f0eafe3273d5c99dcdc",
            "7f6a3c38d3204b35a5455598045c7536",
            "b4b459d783ed4e5cbd30dffb60c450fb",
            "97a9c1ee7e444bf28a2007d12bb6b15e",
            "eb53a979bbeb4d37a6170917f20f7e05",
            "f8300feee1474262b23c5c8c98e67f43",
            "e0550b12362b4362bdd29f462116c956",
            "5b57ce40549445d59c7f86de48109b57",
            "76664fe8073844ada2914011a7001515",
            "58794d4d8814414599c46a42640b266b",
            "538e702a980444c3b18684298540d361",
            "79470505a2b34dc2bf6745c1cd0a793e",
            "f05ac8ce44794b1d90f883541a0f7252",
            "530015905c7a4ee79b897aee4f0768ea",
            "5513847dbf464d6a8268b854f1998d10",
            "e3102b9bceb340328489eddefb7c825c",
            "cb4bd983205b45da82579a6317fc85a3",
            "f066f3f7aafc4d7bbcc75646bf57bd22",
            "1bbadcb769f84bf7b7d220defc0cff77",
            "594ac75ba43c4f5b81525ea5f81e8885",
            "744d223a6e3149a89ebbade951ed27e0",
            "706d2c4f419d404783884cdb5ad83042",
            "331d8a7e051441afa9048fa34cc7c987",
            "4d5cecde050d4821af60ca25df23af6a",
            "b9e31901ee7d4d7ba7753bfa54d20d99",
            "297db9233491414a9c9c5d261c0f78ec",
            "8260fce47c77431abe8d1c96f002b1a5",
            "11ee118068324c79848cce57f8653bea",
            "7ef0b5175fec4b898a43fd1bb4877f50",
            "ea82f8515e564930935279f4c7c5e7dd",
            "2735750b1fe64eacb8ac73bd7008761e",
            "8dc2b1ea752a4343bd3a1db8c5567a39",
            "662d1ecc27b44a659449b44b7cd983d3",
            "83e5f89f1875408faff71ecde58ec02e",
            "5e8810466726409b878d4fc1ab2f6c84",
            "30dc1f30688c48caa6f108d8bfca5c2f",
            "9bf4e33fb1034ce582643311327e2332",
            "8a668622ed5e48ab926cb9faec8b648f",
            "ca291814c539426fb04119b0272b35bc",
            "f2bf69efb1b347a592222569acb41c72",
            "c5a8e5d6cf8044b7addead6024724f21",
            "31f4b2e0d5d541f8ab722f62891e0b84",
            "f7530102d05342f09dab1f2056421a05",
            "61345342587d44d785054cf69fc30839",
            "38099771a2b14bce8cc5d58ebb87427a",
            "c0de310913174a74a2565eba45bcecc0",
            "06a6ca7894404083af08d522cd123aa3",
            "f54106ca4b414142ba8f7a2c0c5297a8",
            "8fb377dbcb76449bb2cfb4ea7df40310",
            "541d7fc28bdc45d59d2943653d91fd6e",
            "c519242363104607ac2b9a5359a7aa80",
            "6d8705652a6e4389a282f9dd8ba2a549",
            "1cdb088fa1a54ecbb1f56a6c98bc756f",
            "542797d75ea943c5b86ca38ab0a53b8e",
            "86b618e87b1e4019b35fb3357f00fe2d",
            "417fd09912974fd6ba7ac698828fd191",
            "eb294bd6bd784a2792d78be92dc28abe",
            "97414d594a804f2bb044694a09ab3917",
            "3f72d5170c7e43faa9a3500d0790b936",
            "45a27e39367e4ccfbb2a9db259d40a28",
            "aa225317c5954e51894eb6b04735407e",
            "d79a52400a854ae29115ce9bbbff1d5c",
            "f0e2f0453aef41a68d89259c681134fc",
            "a6ddd0a2c09b4a73a5015eaf7340df18",
            "346bd39ab10745a5b5f3f281095a7a5d",
            "967325ab7c2f4857bc9cc0482abc68fe",
            "f45fae3675bf4f17883f6d56385ac22d",
            "753b5b4e4167459e934a2ea4022afd14",
            "42031f57db7745deb70cd892b6aebde0",
            "7510370cefe54b3290950def405cdf90",
            "31a580a10e5440a3b33d8157473d6393",
            "d16d006ec4064b50942db8ce35e5a706",
            "3c550e030b36483db9b829113f5f0f71",
            "e91935f0b493446e95f0fdb49812bfd9",
            "4434db387c3747daa09a8a939109f94d",
            "f0106740d1224623ba3f3e233f7e0f07",
            "89fc7eaf0a0a4d648d4139ca67b6cdc4",
            "fe9dd8707c594c538c6297be764da416",
            "b162f89337af4606811be0cf1b34446a",
            "edbdb53c38c84443ba70e30d80ab0f1d"
          ]
        },
        "id": "st7x-r2NHVs2",
        "outputId": "1e3453e1-f783-4422-9d39-f860be6b79fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing GPU memory...\n",
            "\n",
            "Starting Gemma training process...\n",
            "Starting Gemma training setup...\n",
            "Loading Gemma model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a144390a3580457abaaa9da712fcc9f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74efaba1df474425b0ec8174b11df8e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10973e5f20fd4a0dade3fafbf07ba429"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1d5ab5e584244b7a3bc5dcc93c1aeba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1b6807d3f944d749df9a86cbdcb3684"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97a9c1ee7e444bf28a2007d12bb6b15e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5513847dbf464d6a8268b854f1998d10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "297db9233491414a9c9c5d261c0f78ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bf4e33fb1034ce582643311327e2332"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f54106ca4b414142ba8f7a2c0c5297a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f72d5170c7e43faa9a3500d0790b936"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Dataset example:\n",
            "{'question': 'What is machine learning?', 'answer': 'Machine learning is a branch of artificial intelligence that enables computers to learn from data and improve their performance without being explicitly programmed.'}\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7510370cefe54b3290950def405cdf90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LoRA...\n",
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-bb977126c7a6>\", line 135, in <cell line: 133>\n",
            "    model, tokenizer = train_gemma()\n",
            "  File \"<ipython-input-12-bb977126c7a6>\", line 107, in train_gemma\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Qwen-1.5**"
      ],
      "metadata": {
        "id": "AGhug9gYHXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_qwen():\n",
        "    print(\"Starting Qwen training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Qwen model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"Qwen/Qwen1.5-0.5B\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        \"Qwen/Qwen1.5-0.5B\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple dataset for summarization\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"text\": \"The sun is a massive ball of gas primarily composed of hydrogen and helium. It undergoes nuclear fusion in its core, releasing enormous amounts of energy in the form of light and heat. This energy travels through space and reaches Earth, providing the heat and light necessary for life.\",\n",
        "            \"summary\": \"The sun is a giant gas sphere that produces energy through nuclear fusion, providing Earth with essential light and heat.\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Artificial Intelligence (AI) is a field of computer science focused on creating intelligent machines that can perform tasks typically requiring human intelligence. These tasks include visual perception, speech recognition, decision-making, and language translation. AI systems learn from experience using machine learning algorithms.\",\n",
        "            \"summary\": \"AI is computer technology that mimics human intelligence, performing tasks like recognition, decision-making, and translation through learning algorithms.\"\n",
        "        },\n",
        "        {\n",
        "            \"text\": \"Climate change refers to long-term shifts in global weather patterns and average temperatures. It's primarily caused by human activities releasing greenhouse gases into the atmosphere. These gases trap heat, leading to global warming, rising sea levels, and extreme weather events.\",\n",
        "            \"summary\": \"Climate change involves long-term weather and temperature changes, mainly caused by human-produced greenhouse gases leading to global warming.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data\n",
        "    def format_data(example):\n",
        "        return f\"<|im_start|>user\\nText to summarize: {example['text']}<|im_end|>\\n<|im_start|>assistant\\nSummary: {example['summary']}<|im_end|>\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./qwen_results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        save_steps=3,\n",
        "        max_steps=10\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./qwen_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_qwen(model, tokenizer, text):\n",
        "    print(f\"\\nTesting with text: {text[:100]}...\")  # Print first 100 chars\n",
        "    prompt = f\"<|im_start|>user\\nText to summarize: {text}<|im_end|>\\n<|im_start|>assistant\\nSummary:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Clear GPU memory\n",
        "print(\"Clearing GPU memory...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nStarting Qwen training process...\")\n",
        "    model, tokenizer = train_qwen()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_texts = [\n",
        "        \"Neural networks are computational systems inspired by biological neural networks in human brains. They consist of interconnected nodes that process and transmit information, allowing the system to learn patterns and make predictions.\",\n",
        "        \"The water cycle, also known as the hydrologic cycle, describes the continuous movement of water on Earth. Water evaporates from surfaces, forms clouds, falls as precipitation, and flows through rivers back to the oceans.\"\n",
        "    ]\n",
        "\n",
        "    for text in test_texts:\n",
        "        result = test_qwen(model, tokenizer, text)\n",
        "        print(f\"\\nOriginal Text: {text[:100]}...\")\n",
        "        print(f\"Generated Summary: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./qwen_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Optional: Convert for Ollama\n",
        "def convert_for_ollama():\n",
        "    print(\"\\nConverting for Ollama...\")\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    %cd llama.cpp\n",
        "    !pip install -r requirements.txt\n",
        "    !python convert-lora-to-ggml.py ../qwen_adapter/adapter_config.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a10a55da1d614198b5df2a010ee9b249",
            "443d058ac1c94ec5a658bd53c199f47d",
            "c7f2858a9360485ba1e683266659734b",
            "13673a161f1b412a922b1efc2299c246",
            "2e4b034e7e6047dabebcf3ef6ada88bc",
            "b35bfe29b2a7479db75c7fecaff6ce31",
            "ca603beecd8c4188bae055f3041f37eb",
            "470226ed23cc45a3881aa0cf141af72b",
            "08fcdf0d0bf042ab8962ed25a9cdf0f1",
            "aa5780cd672a464992fd576901b6a376",
            "038276d86c3d48a897c4d69a5bb6a242",
            "77f9f4980f6c431b95533d83a10a2e6d",
            "b0b2e545baae453191cf9cc05599df1e",
            "8d77f092fbd24775ba858c977fbf7d70",
            "b3cacc863aab4f7f93f7c5e6867b9cde",
            "37dbeacabcfb4d11903f3c4157840f49",
            "759575d469554ed98d0edd89ba192e5c",
            "c8d941d705044842b79d9d8902188b21",
            "6ae05836655c4e8996e693348ecd21d0",
            "3f0f2f7f5b3f45c0a16b97235df08a75",
            "2d7bd3a0bf734286a1f120d4c1f43a2b",
            "c2b8b8440a8842b5a21f155b923acc15",
            "cdf6c1fad3a243be9264e12d0a3dbd74",
            "4174a1309601415d88d1fb47c05683e9",
            "beb22a6c27d941cfbad3e2c704c197df",
            "e62385ef781d4eb090c85273a50d72bb",
            "e8ecc8c0506a4f33af963943c9b9f3a5",
            "e5651ebb1b5c4f769c6ccc7f0e69a8a3",
            "19c37fabbec04044b96d389e95314e6a",
            "706bb692e03c462a9fc0f51bb6109fe8",
            "c5de8fc2243140c8b839816160731a49",
            "bee5455181bf42e8b0e67d5ef8d6bede",
            "79321d5a2ae24c5e85662cb683f466a6",
            "c8814b4528244041b768f0323f1233bc",
            "e31ba4808b1843079d2e2b9645b98aab",
            "6faea96c03834b13ac15a79c459c1db2",
            "2cf1d9e6145d4f91bca7a808aaea0de3",
            "a53a875d407443278d8292394c8285e1",
            "77fbd595e62e4e4cac7b3da116dd389e",
            "a567990e02e34ade971006ef81726a0a",
            "366fb2127b294d39824a7059a10ee7ec",
            "fe8021f0366d49f7b9026af4a5ff9691",
            "8fd532dfa52c429187b98208382cb77d",
            "2ddc33c2aba84342b582fc1309edf1bd",
            "4da27848345845fcb350928bd3de72b7",
            "f7965e2563924b8c906dce70c8061663",
            "fd635c7d4ce94a24960d465a6fc1cfe8",
            "74a706a7502b44fcb238829dc08ba4b1",
            "bfe554ef97fa40d7873e70ce58e47957",
            "831de406b6b8450e996f4f7b4637dfbf",
            "67fa416a411f4e4d9dc371bd33fa046b",
            "bfd81a4d36c54fc0827da72cf62f94cb",
            "cbc4487a41144235a77d35c0cfa99ea8",
            "d28307b96b174836af0eed4fb0b3d64a",
            "87c649f343994f5eae53015e1fb79e47",
            "5b8a4b1889b8458aa1b6424cc23a2f30",
            "9530ed32242b4e1cb454316f3c41911f",
            "e60a92f4436445af91cb6964f910c8e5",
            "8af05c5ed8614462ad770dcbef7d9c18",
            "2f21990dfd5a4cad83e61c675227729d",
            "efa865a3c87f4da19e761055a029d988",
            "3bc1195b6a834235ab717fd9c35aaa74",
            "12786f16a0814fccb9415ec7b5ecd441",
            "3091179bef4f4d9a8d9ed3b25aa64fc9",
            "6349b110c6514954b65b10532bde5cfd",
            "facb1f1072134053a5861973bb0b83ba",
            "338ceac9f13f49cf9b556ffbdcf9c395",
            "8b66cf49f8bd4b208e385310da5a6103",
            "f7abb9b7b71446cea39cfe75ff0e1ec8",
            "103fbeffa314496db4b9ef6d4dfd1591",
            "4c4c407059ce4be88ec301a57eb2488c",
            "791bd8ca7258424994bd63b5793a6cf6",
            "a5f9ee8e01194524895a744eebcec3c3",
            "af3c4796a69d4df59228077abe00854a",
            "70af9ccb3b1a4119a92a3a271db3b3eb",
            "92ce85d57fde467a8c9cc0d5f698e913",
            "242ee10cfae848519df6a7f216593f4b",
            "780eb17b2903466985836009de01dec9",
            "f865d08448844cb981cd4735071fe27b",
            "ca4e5d3faf63495085c98e5ba3822fb9",
            "da0e2a0ad8a340d4a1c6ae6e6217e0b9",
            "6576542d1eb14e51a4a8109f74b98931",
            "96a6c64dd774460688eb7e5508e3172b",
            "2f04f405e24e4cd3b568df7ec7356484",
            "712f301aca0f43379a02f650e72dbccc",
            "d462392e7d064b718f0c93a63975b229",
            "cb4ae771ef0d443c91ee5972c3eae10a",
            "afbd7f59932743eea650eff965f2fbcb"
          ]
        },
        "id": "JpV04C7wHYhH",
        "outputId": "b07b3760-b7ac-4981-b137-a02510fb94cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing GPU memory...\n",
            "\n",
            "Starting Qwen training process...\n",
            "Starting Qwen training setup...\n",
            "Loading Qwen model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a10a55da1d614198b5df2a010ee9b249"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77f9f4980f6c431b95533d83a10a2e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdf6c1fad3a243be9264e12d0a3dbd74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8814b4528244041b768f0323f1233bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4da27848345845fcb350928bd3de72b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b8a4b1889b8458aa1b6424cc23a2f30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "338ceac9f13f49cf9b556ffbdcf9c395"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dataset...\n",
            "Dataset example:\n",
            "{'text': 'The sun is a massive ball of gas primarily composed of hydrogen and helium. It undergoes nuclear fusion in its core, releasing enormous amounts of energy in the form of light and heat. This energy travels through space and reaches Earth, providing the heat and light necessary for life.', 'summary': 'The sun is a giant gas sphere that produces energy through nuclear fusion, providing Earth with essential light and heat.'}\n",
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "780eb17b2903466985836009de01dec9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up LoRA...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up training arguments...\n",
            "Initializing trainer...\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An error occurred: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-13-7c3b3ffeebf2>\", line 135, in <cell line: 133>\n",
            "    model, tokenizer = train_qwen()\n",
            "  File \"<ipython-input-13-7c3b3ffeebf2>\", line 107, in train_qwen\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1938, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
            "    return self.call_event(\"on_train_begin\", args, state, control)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
            "    self.setup(args, state, model, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 833, in setup\n",
            "    self._wandb.init(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1270, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py\", line 161, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 1255, in init\n",
            "    wi.setup(kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 305, in setup\n",
            "    wandb_login._login(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 347, in _login\n",
            "    wlogin.prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 274, in prompt_api_key\n",
            "    key, status = self._prompt_api_key()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\", line 253, in _prompt_api_key\n",
            "    key = apikey.prompt_api_key(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\", line 164, in prompt_api_key\n",
            "    key = input_callback(api_ask).strip()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 164, in prompt\n",
            "    value = prompt_func(prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/termui.py\", line 147, in prompt_func\n",
            "    raise Abort() from None\n",
            "click.exceptions.Abort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mistral-7B-Instruct-v0.2**"
      ],
      "metadata": {
        "id": "TU1gUYNWI7hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "def train_mistral():\n",
        "    print(\"Starting Mistral training setup...\")\n",
        "\n",
        "    # Setup quantization config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Load model\n",
        "    print(\"Loading Mistral model...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"mistralai/Mistral-7B-v0.1\",\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create simple instruction dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    train_data = [\n",
        "        {\n",
        "            \"instruction\": \"Explain quantum computing\",\n",
        "            \"response\": \"Quantum computing uses quantum mechanics principles like superposition and entanglement to perform computations. Unlike classical computers that use bits (0 or 1), quantum computers use quantum bits or qubits that can exist in multiple states simultaneously, potentially solving complex problems much faster.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"What is machine learning?\",\n",
        "            \"response\": \"Machine learning is a branch of artificial intelligence where computer systems learn and improve from experience without explicit programming. They identify patterns in data to make predictions and decisions, becoming more accurate over time.\"\n",
        "        },\n",
        "        {\n",
        "            \"instruction\": \"How does blockchain work?\",\n",
        "            \"response\": \"Blockchain is a decentralized digital ledger that records transactions across a network of computers. Each block contains transaction data and links to the previous block, creating a chain. The system is secure because changes require consensus from the network, making it difficult to alter records.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    dataset = Dataset.from_list(train_data)\n",
        "\n",
        "    print(\"Dataset example:\")\n",
        "    print(dataset[0])\n",
        "\n",
        "    # Format data with Mistral chat template\n",
        "    def format_data(example):\n",
        "        return f\"<s>[INST] {example['instruction']} [/INST] {example['response']}</s>\"\n",
        "\n",
        "    # Tokenize dataset\n",
        "    print(\"Tokenizing dataset...\")\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda x: tokenizer(\n",
        "            format_data(x),\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None\n",
        "        ),\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    print(\"Setting up LoRA...\")\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"o_proj\"\n",
        "        ],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()  # Print trainable parameters\n",
        "\n",
        "    print(\"Setting up training arguments...\")\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./mistral_results\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=1,\n",
        "        save_steps=3,\n",
        "        max_steps=10,\n",
        "        fp16=True,  # Use mixed precision\n",
        "        optim=\"paged_adamw_8bit\"  # Use 8-bit optimizer\n",
        "    )\n",
        "\n",
        "    print(\"Initializing trainer...\")\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving model...\")\n",
        "    # Save the fine-tuned model\n",
        "    model.save_pretrained(\"./mistral_adapter\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Test function\n",
        "def test_mistral(model, tokenizer, instruction):\n",
        "    print(f\"\\nTesting with instruction: {instruction}\")\n",
        "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=200,\n",
        "        temperature=0.7,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Memory management function\n",
        "def clear_memory():\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    print(\"\\nClearing memory...\")\n",
        "    clear_memory()\n",
        "\n",
        "    print(\"\\nStarting Mistral training process...\")\n",
        "    model, tokenizer = train_mistral()\n",
        "\n",
        "    # Test the model\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_instructions = [\n",
        "        \"Explain how solar panels work\",\n",
        "        \"What is the theory of evolution?\",\n",
        "        \"How does the internet work?\"\n",
        "    ]\n",
        "\n",
        "    for instruction in test_instructions:\n",
        "        result = test_mistral(model, tokenizer, instruction)\n",
        "        print(f\"\\nInstruction: {instruction}\")\n",
        "        print(f\"Response: {result}\")\n",
        "\n",
        "    # Save to Drive\n",
        "    print(\"\\nSaving to Google Drive...\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !cp -r ./mistral_adapter '/content/drive/MyDrive/'\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Convert for Ollama\n",
        "def convert_for_ollama():\n",
        "    !git clone https://github.com/ggerganov/llama.cpp\n",
        "    %cd llama.cpp\n",
        "    !pip install -r requirements.txt\n",
        "    !python convert-lora-to-ggml.py ../mistral_adapter/adapter_config.json\n",
        "\n",
        "    # Create Modelfile with proper string formatting\n",
        "    modelfile_content = '''\n",
        "FROM mistral:latest\n",
        "ADAPTER ./ggml-adapter-model.bin\n",
        "TEMPLATE \"\"\"<s>[INST] {{ .Prompt }} [/INST]\"\"\"\n",
        "PARAMETER stop \"[INST]\"\n",
        "PARAMETER stop \"</s>\"\n",
        "'''\n",
        "\n",
        "    with open(\"Modelfile\", \"w\") as f:\n",
        "        f.write(modelfile_content)\n",
        "\n",
        "    print(\"Created Modelfile for Ollama\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSSkpKsSI8cJ",
        "outputId": "092ef1e3-793e-43b9-d30f-6494cbcf0ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clearing memory...\n",
            "GPU Memory available: 15.84 GB\n",
            "\n",
            "Starting Mistral training process...\n",
            "Starting Mistral training setup...\n",
            "Loading Mistral model...\n",
            "\n",
            "An error occurred: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n",
            "403 Client Error. (Request ID: Root=1-67229ce1-08c0444f27bd206e0e06af16;283224c8-c909-4d2c-9460-5e77c7cca34e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
            "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 402, in cached_file\n",
            "    resolved_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1240, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1347, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1855, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1752, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1674, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 376, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 400, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 321, in hf_raise_for_status\n",
            "    raise GatedRepoError(message, response) from e\n",
            "huggingface_hub.utils._errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-67229ce1-08c0444f27bd206e0e06af16;283224c8-c909-4d2c-9460-5e77c7cca34e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
            "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-15-9fd45cada668>\", line 147, in <cell line: 142>\n",
            "    model, tokenizer = train_mistral()\n",
            "  File \"<ipython-input-15-9fd45cada668>\", line 19, in train_mistral\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 524, in from_pretrained\n",
            "    config, kwargs = AutoConfig.from_pretrained(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 976, in from_pretrained\n",
            "    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 632, in get_config_dict\n",
            "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 689, in _get_config_dict\n",
            "    resolved_config_file = cached_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 420, in cached_file\n",
            "    raise EnvironmentError(\n",
            "OSError: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n",
            "403 Client Error. (Request ID: Root=1-67229ce1-08c0444f27bd206e0e06af16;283224c8-c909-4d2c-9460-5e77c7cca34e)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
            "Access to model mistralai/Mistral-7B-v0.1 is restricted and you are not in the authorized list. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xSd0Ppy4JUzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **b.) Continued pretraining**"
      ],
      "metadata": {
        "id": "OMwfB_aDOE0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHVNcustOPNb",
        "outputId": "9f552c61-e011-409e-f3d3-ee74d0de4a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2024.10.7-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth-zoo (from unsloth)\n",
            "  Downloading unsloth_zoo-2024.10.5-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (2.5.0+cu121)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.1)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.1)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: transformers<4.45.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.44.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.0.2)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.44.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.34.2)\n",
            "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.11.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.13.2)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth) (0.24.7)\n",
            "Collecting hf-transfer (from unsloth)\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.1->unsloth) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.10.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.45.0->unsloth) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<4.45.0->unsloth) (0.19.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth) (13.9.3)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth) (0.2.0)\n",
            "Downloading unsloth-2024.10.7-py3-none-any.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2024.10.5-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: triton, shtab, hf-transfer, xformers, tyro, trl, unsloth-zoo, unsloth\n",
            "Successfully installed hf-transfer-0.1.8 shtab-1.7.1 triton-3.1.0 trl-0.11.1 tyro-0.8.14 unsloth-2024.10.7 unsloth-zoo-2024.10.5 xformers-0.0.28.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from getpass import getpass\n",
        "\n",
        "# Prompt for Hugging Face token securely\n",
        "hf_token = getpass(\"Please enter your Hugging Face token: \")\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=hf_token)\n",
        "\n",
        "# Now load your model\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B\"  # Replace with the model name\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=512,\n",
        "    dtype=\"float16\",\n",
        "    load_in_4bit=True,\n",
        "    token=hf_token  # Pass token for private model access\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "G1-nmr1jOWvD",
        "outputId": "5d5a26e6-0f6e-4374-ff8f-96fd048b24ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your Hugging Face token: ··········\n",
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mistralai/Mistral-7B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-64aaccdd55b5>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mistralai/Mistral-7B\"\u001b[0m  \u001b[0;31m# Replace with the model name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0;34mf\"to obtain the latest transformers build, then restart this session.\"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 ) \n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoconfig_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpeft_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mistralai/Mistral-7B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define your model name (e.g., a smaller Hugging Face model for Colab)\n",
        "model_name = \"mistralai/Mistral-7B\"  # Replace with the desired model\n",
        "\n",
        "# Load the model and tokenizer with settings optimized for Colab\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=512,\n",
        "    dtype=\"float16\",  # Helps reduce memory usage\n",
        "    load_in_4bit=True,  # Further reduces memory load\n",
        "    token=os.environ[\"HUGGINGFACE_TOKEN\"]  # Fetches token from environment\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "unUfXYIsQaBH",
        "outputId": "ac206717-0aa5-4f6a-f62e-9c63e3d1e6fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-138fa4627c1c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float16\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Helps reduce memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mload_in_4bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Further reduces memory load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HUGGINGFACE_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Fetches token from environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXXuqaNpRsC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}